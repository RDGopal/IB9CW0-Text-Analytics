{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "provenance": []
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "cells": [
    {
      "cell_type": "markdown",
      "source": [
        "# Small Language Models (SLM)\n",
        "\n",
        "Microsoft recently released the Phi-3 family, a suite of small language models (SLMs) designed to bring advanced generative AI technology to a broader set of platforms.These models are open source and available for public use. They can be accessed through platforms like Microsoft Azure AI Studio, Hugging Face, and Ollama. Phi-3 models are trained with a mix of web data and synthetic data, focusing on general knowledge and specialized skills for complex problem-solving.\n",
        "\n",
        "# Ollama\n",
        "Ollama is a streamlined tool for running open-source LLMs locally. Local LLMs controlled from Ollama provide a convenient means for developers to experiment and iterate on their models without relying on external resources."
      ],
      "metadata": {
        "id": "lbcsw1jiNPHI"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "# Working with Ollama and Phi-latest"
      ],
      "metadata": {
        "id": "M6-tJRjSiW3Z"
      }
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "9BMEGxvmgt3O"
      },
      "outputs": [],
      "source": [
        "# Install Ollama v0.1.30\n",
        "!curl https://ollama.ai/install.sh | sed 's#https://ollama.ai/download#https://github.com/jmorganca/ollama/releases/download/v0.1.30#' | sh"
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "## Code Explanation\n",
        "The code snippet above is a shell command line that combines several Unix commands to download and execute an installation script from the internet. Here's a breakdown of what each part of the command does:\n",
        "\n",
        "* !curl https://ollama.ai/install.sh:\n",
        "\n",
        "curl is a command-line tool used to transfer data from or to a server using various protocols. In this case, it is used to download a script from the specified URL (https://ollama.ai/install.sh).\n",
        "The ! at the beginning indicates that this command is run in a context where ! is used to execute shell commands, such as in a Jupyter notebook.\n",
        "\n",
        "* | sed\n",
        "\n",
        "The pipe | is used to pass the output of the previous command (curl) as input to the next command. sed is a stream editor for filtering and transforming text.\n",
        "\n",
        "* s#https://ollama.ai/download#https://github.com/jmorganca/ollama/releases/download/v0.1.30#\n",
        "\n",
        "is a substitution command that replaces the text https://ollama.ai/download with https://github.com/jmorganca/ollama/releases/download/v0.1.30. This modifies the URL in the script being piped in, to redirect the download location to a different server or to use a specific version of the software.\n",
        "\n",
        "* | sh:\n",
        "\n",
        "This part of the command takes the modified script (after sed has replaced the URL) and pipes it to sh, which is a command interpreter that executes the commands in the script. Essentially, this means that after modifying the installation script, it is directly executed."
      ],
      "metadata": {
        "id": "qVTWBvhPj3wF"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# Setup the model as a global variable\n",
        "OLLAMA_MODEL='phi:latest'\n",
        "\n",
        "# Add the model to the environment of the operating system\n",
        "import os\n",
        "os.environ['OLLAMA_MODEL'] = OLLAMA_MODEL\n",
        "!echo $OLLAMA_MODEL # print the global variable to check it saved\n",
        "\n",
        "import subprocess\n",
        "import time\n",
        "\n",
        "# Start ollama on the server (\"serve\")\n",
        "command = \"nohup ollama serve&\" # \"nohup\" and \"&\" means run in the background\n",
        "\n",
        "# Use subprocess.Popen to run the command\n",
        "process = subprocess.Popen(command,\n",
        "                            shell=True,\n",
        "                            stdout=subprocess.PIPE,\n",
        "                            stderr=subprocess.PIPE)\n",
        "\n",
        "print(\"Process ID:\", process.pid) # print the process ID\n",
        "time.sleep(5)  # Makes Python wait for 5 seconds\n",
        "\n",
        "!ollama -v # print the Ollama version number as a check"
      ],
      "metadata": {
        "id": "63m9Tah6hU0a"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "##Code Explanation\n",
        "\n",
        "### Part 1: Setting and Using an Environment Variable\n",
        "\n",
        "Global Variable Definition: OLLAMA_MODEL='phi:latest' sets a Python variable OLLAMA_MODEL with the value 'phi:latest'.\n",
        "\n",
        "Setting Environment Variable: The variable is then added to the system's environment variables using os.environ['OLLAMA_MODEL'] = OLLAMA_MODEL. This allows other programs and subprocesses in the same environment to access this variable.\n",
        "\n",
        "Checking the Variable: !echo $OLLAMA_MODEL prints the value of the OLLAMA_MODEL environment variable to verify it was set correctly. In notebooks and similar environments, ! is used to execute shell commands.\n",
        "\n",
        "### Part 2: Running a Background Process\n",
        "\n",
        "Running the Server: nohup ollama serve& starts the ollama serve command in the background. nohup is used to run the process without hanging up, even if the terminal is closed. The & at the end puts the command in the background so that the terminal or script can continue running other commands.\n",
        "\n",
        "subprocess.Popen: This function is used to execute the command in a new subprocess, allowing the script to continue running without waiting for the command to complete.\n",
        "\n",
        "stdout=subprocess.PIPE and stderr=subprocess.PIPE capture the standard output and standard error of the command, respectively.\n",
        "\n",
        "Process ID and Sleep: The process ID (process.pid) of the newly started server is printed, and time.sleep(5) pauses the Python script for 5 seconds to allow the server some time to initialize.\n",
        "\n",
        "### Part 3: Checking the Version\n",
        "\n",
        "Version Check: Finally, !ollama -v executes another shell command to check and print the version of the ollama tool, ensuring that the right version is running or simply as a confirmation of the setup.\n",
        "\n",
        "### Summary\n",
        "\n",
        "This script is a practical example of how to programmatically control and verify server processes within a Python environment, especially in contexts like Jupyter notebooks. It sets up necessary configurations, starts a background process, and verifies that the environment is correctly configured."
      ],
      "metadata": {
        "id": "WkYWiNN0kNod"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "## Run the SLM"
      ],
      "metadata": {
        "id": "iCjbnbBfWk6V"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# Query the model via the command line\n",
        "# First time running it will \"pull\" (import) the model\n",
        "!ollama run $OLLAMA_MODEL \"Give me short summary of the city of Birmingham, UK.\""
      ],
      "metadata": {
        "id": "OLtEcFvdg83O"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "prompt = \"Give me short summary of the internet\"  # @param {type:\"string\"}\n",
        "!ollama run $OLLAMA_MODEL {prompt}"
      ],
      "metadata": {
        "id": "5JSsqpkCh_ZA"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "Reviews =  [\"Great service and friendly staff.\", \"The wait time was too long, unimpressed.\", \"Decent, but could be better.\"]"
      ],
      "metadata": {
        "id": "rY00j2bzc_s7"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "prompt = f\"Please analyze the following text and give me your best view on whether the review is positive, negative, or neutral in tone: '{Reviews[1]}'\"\n",
        "!ollama run $OLLAMA_MODEL {prompt}\n"
      ],
      "metadata": {
        "id": "BqY01fBxelqj"
      },
      "execution_count": null,
      "outputs": []
    }
  ]
}