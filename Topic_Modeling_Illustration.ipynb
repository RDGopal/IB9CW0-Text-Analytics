{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "provenance": [],
      "authorship_tag": "ABX9TyOiFwH6EOrMQgt941y67x/0",
      "include_colab_link": true
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/RDGopal/IB9CW0-Text-Analytics/blob/main/Topic_Modeling_Illustration.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "# Topic Modeling Illustration\n",
        "Topic modeling is a technique used in natural language processing to discover latent topics in a collection of documents.\n",
        "Topic modeling a variety of applications, such as text classification, information retrieval, predictive modeling and sentiment analysis. For example, in text classification, it can be used to classify documents into different topics based on the distribution of words in the documents. In information retrieval, it can be used to identify relevant documents based on the topics that are most relevant to the userâ€™s query.\n"
      ],
      "metadata": {
        "id": "Nz42gcg4TY19"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "# LDA\n",
        "## Create a random collection of documents and terms\n",
        "We start by creating a vocabulary of terms and a random collection of documents. In this example, we use a vocabulary of 15 terms and generate 4 documents, each with 6 random terms.\n"
      ],
      "metadata": {
        "id": "jjXJvzH0Tr9p"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "import numpy as np\n",
        "import pandas as pd\n",
        "import matplotlib.pyplot as plt\n",
        "from gensim import corpora, models"
      ],
      "metadata": {
        "id": "MpR-5N8rTxm5"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "np.random.seed(100)  # for reproducibility\n",
        "\n",
        "# Define the vocabulary\n",
        "vocabulary = [\"the\", \"cat\", \"sat\", \"on\", \"mat\", \"and\", \"licked\", \"its\", \"paw\", \"dog\", \"chased\", \"ball\", \"barked\", \"loudly\", \"sun\"]\n",
        "\n",
        "# Define the number of documents, words per document, and number of topics\n",
        "num_docs = 4\n",
        "num_words = 6\n",
        "k = 4\n",
        "\n",
        "# Simulate words in each document\n",
        "documents = [np.random.choice(vocabulary, num_words, replace=True) for _ in range(num_docs)]\n",
        "\n",
        "for i, doc in enumerate(documents):\n",
        "    print(f\"Document {i+1}: {doc}\")"
      ],
      "metadata": {
        "id": "k0JoJO7MT8vw"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "##Display documents and terms\n",
        "Next, we display the documents and terms.\n"
      ],
      "metadata": {
        "id": "KirM2qEFUPYI"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "fig, axes = plt.subplots(1, num_docs, figsize=(10, 2))\n",
        "\n",
        "for i, ax in enumerate(axes):\n",
        "    ax.axis('off')\n",
        "    ax.text(0.5, 0.5, '\\n'.join(documents[i]), fontsize=12, ha='center', va='center')\n",
        "\n",
        "plt.show()\n"
      ],
      "metadata": {
        "id": "8M1BkpjnTqw6"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "##Assign terms to topics\n",
        "How do we assign terms to topics? Letâ€™s start with a random assignment. We randomly assign each term to a topic and create a dataframe to represent the topic assignments.\n"
      ],
      "metadata": {
        "id": "5CEmzil4UcmH"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# Randomly assign each word to a topic\n",
        "word_topics = np.random.randint(0, k, len(vocabulary))\n",
        "topic_colors = [\"red\", \"blue\", \"green\", \"pink\"]\n",
        "\n",
        "# Create a DataFrame to represent topic assignments\n",
        "df_word_topic = pd.DataFrame({\n",
        "    'vocabulary': vocabulary,\n",
        "    'word_topics': word_topics,\n",
        "    'Color': [topic_colors[i] for i in word_topics]\n",
        "})\n",
        "\n",
        "print(df_word_topic)\n"
      ],
      "metadata": {
        "id": "ZElU67wGUlSA"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "##Plot the documents and terms by topic\n",
        "We plot the documents and terms, coloring each term by its assigned topic.\n",
        "We compute the number of different topics represented in each document, and the product of the number of topics across all documents.\n"
      ],
      "metadata": {
        "id": "XDbFPIbsUx_Q"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "import matplotlib.pyplot as plt\n",
        "\n",
        "# Define colors for the topics\n",
        "topic_colors = [\"red\", \"blue\", \"green\", \"pink\"]\n",
        "\n",
        "# Create the figure and axes based on the number of documents\n",
        "fig, axes = plt.subplots(1, num_docs, figsize=(15, 3))\n",
        "\n",
        "# Plot each document\n",
        "for i, ax in enumerate(axes):\n",
        "    ax.axis('off')  # Turn off axis\n",
        "    doc_words = documents[i]\n",
        "    word_labels = []\n",
        "    word_color = []\n",
        "\n",
        "    # Retrieve colors and vocabulary for the document\n",
        "    for word in doc_words:\n",
        "        idx = vocabulary.index(word)\n",
        "        word_labels.append(word)\n",
        "        word_color.append(topic_colors[word_topics[idx]])\n",
        "\n",
        "    # Define the y positions to space words evenly in the plot\n",
        "    y_positions = np.linspace(0.1, 0.9, num_words)\n",
        "\n",
        "    # Plot each word with its corresponding color\n",
        "    for label, color, y_pos in zip(word_labels, word_color, y_positions):\n",
        "        ax.text(0.5, y_pos, label, color=color, ha='center', fontsize=12, family='sans')\n",
        "\n",
        "plt.tight_layout()\n",
        "plt.show()\n"
      ],
      "metadata": {
        "id": "xXhwQIeqWAb3"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "##Compute the number of topics represented in each document\n",
        "We compute the number of different topics represented in each document, and the product of the number of topics across all documents."
      ],
      "metadata": {
        "id": "NDsp4fdDWOvY"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# Initialize the product of distinct topics across documents\n",
        "prod = 1\n",
        "\n",
        "for i, doc in enumerate(documents):\n",
        "    topics_in_doc = np.unique([word_topics[vocabulary.index(word)] for word in doc])\n",
        "    num_topics = len(topics_in_doc)\n",
        "    print(f\"Document {i+1} has {num_topics} different topics.\")\n",
        "    prod *= num_topics\n",
        "\n",
        "print(f\"The product of the numbers of different topics represented in each of the documents is {prod}.\")\n"
      ],
      "metadata": {
        "id": "gGwiFqW0U4cv"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "##Random Reassignment of a Word\n",
        "In the previous section, we randomly assigned words to topics and computed the product of the number of distinct topics represented in each document. However, this random assignment is unlikely to be optimal. Ideally, we would like to have as few topics in each document as possible. We can improve the assignment by randomly picking a word and reassigning it to a different topic. We repeat this process multiple times and keep the word-topic assignment that has the lowest product of the number of distinct topics represented in each document.\n",
        "\n",
        "First, we will define a function in Python to randomly reassign a word to a different topic within a document.\n"
      ],
      "metadata": {
        "id": "BiAbXFEDVKVH"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "import numpy as np\n",
        "\n",
        "def topicassign(documents, word_topics, vocabulary, k):\n",
        "    # Choose a random word from the entire vocabulary\n",
        "    word_index = np.random.randint(len(vocabulary))\n",
        "    word = vocabulary[word_index]\n",
        "    old_topic = word_topics[word_index]\n",
        "\n",
        "    # Choose a new topic different from the current one\n",
        "    possible_topics = list(set(range(k)) - {old_topic})\n",
        "    new_topic = np.random.choice(possible_topics)\n",
        "\n",
        "    # Update the topic of the chosen word\n",
        "    word_topics[word_index] = new_topic\n",
        "    return word_topics\n",
        "\n",
        "def calculate_product(documents, word_topics, vocabulary):\n",
        "    # Calculate the product of distinct topics in each document\n",
        "    product = 1\n",
        "    for doc in documents:\n",
        "        topics_in_doc = set(word_topics[vocabulary.index(word)] for word in doc)\n",
        "        product *= len(topics_in_doc)\n",
        "    return product"
      ],
      "metadata": {
        "id": "7DCMefQBXHB5"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "###Find the Optimal Word-Topic Assignment\n",
        "We will loop through the topicassign function multiple times, trying to minimize the product of the number of distinct topics represented in each document."
      ],
      "metadata": {
        "id": "8WmDGgohXdGO"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "np.random.seed(100)  # For reproducibility\n",
        "\n",
        "# Simulate initial topics\n",
        "word_topics = np.random.randint(0, k, len(vocabulary))\n",
        "\n",
        "# Calculate the initial product of distinct topics\n",
        "initial_product = calculate_product(documents, word_topics, vocabulary)\n",
        "print(\"Initial product of distinct topics:\", initial_product)\n",
        "\n",
        "best_product = initial_product\n",
        "best_assignment = word_topics.copy()\n",
        "\n",
        "# Iterative process to optimize topic assignment\n",
        "for _ in range(1000):\n",
        "    new_word_topics = topicassign(documents, word_topics.copy(), vocabulary, k)\n",
        "    new_product = calculate_product(documents, new_word_topics, vocabulary)\n",
        "    if new_product < best_product:\n",
        "        best_product = new_product\n",
        "        best_assignment = new_word_topics.copy()\n",
        "\n",
        "# Print the optimized product and assignment\n",
        "print(\"Optimized product of distinct topics:\", best_product)\n",
        "print(\"Optimized assignment:\", best_assignment)\n"
      ],
      "metadata": {
        "id": "UG20WqbeXgiq"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "###Display the optimized word-topic assignment\n",
        "We display the optimized word-topic assignment and plot the documents and terms by topic.\n"
      ],
      "metadata": {
        "id": "SMqqd6uyXo0G"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "fig, axes = plt.subplots(1, num_docs, figsize=(15, 3))\n",
        "\n",
        "for i, ax in enumerate(axes):\n",
        "    ax.axis('off')\n",
        "    doc_words = documents[i]\n",
        "    word_labels = []\n",
        "    word_color = []\n",
        "\n",
        "    for word in doc_words:\n",
        "        idx = vocabulary.index(word)\n",
        "        word_labels.append(word)\n",
        "        word_color.append(topic_colors[word_topics[idx]])\n",
        "\n",
        "    y_positions = np.linspace(0.1, 0.9, num_words)\n",
        "\n",
        "    for label, color, y_pos in zip(word_labels, word_color, y_positions):\n",
        "        ax.text(0.5, y_pos, label, color=color, ha='center', fontsize=12, family='sans')\n",
        "\n",
        "plt.tight_layout()\n",
        "plt.show()\n"
      ],
      "metadata": {
        "id": "YdX_4kztXvov"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "# Latent Semantic Analysis\n",
        "To illustrate the Latent Semantic Analysis (LSA) method using Singular Value Decomposition (SVD) from first principles, we will follow these steps:\n",
        "\n",
        "Create a Term-Document Matrix: This matrix represents the frequency of each term in each document.\n",
        "\n",
        "Apply SVD: Decompose the term-document matrix to identify patterns in the relationships between the terms and documents.\n",
        "\n",
        "Interpret the Results: Explain how the decomposition reflects latent topics within the data."
      ],
      "metadata": {
        "id": "oOBQ0P0pbJMA"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "##Create a Term-Document Matrix\n",
        "First, we construct a term-document matrix where each row represents a term from our vocabulary, and each column represents a document. The values in this matrix are the frequencies of the terms in the documents."
      ],
      "metadata": {
        "id": "uAbqKeQzbai9"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "import numpy as np\n",
        "import pandas as pd\n",
        "\n",
        "np.random.seed(100)  # for reproducibility\n",
        "\n",
        "# Define the vocabulary\n",
        "vocabulary = [\"the\", \"cat\", \"sat\", \"on\", \"mat\", \"and\", \"licked\", \"its\", \"paw\", \"dog\", \"chased\", \"ball\", \"barked\", \"loudly\", \"sun\"]\n",
        "\n",
        "# Define the number of documents, words per document, and number of topics\n",
        "num_docs = 4\n",
        "num_words = 6\n",
        "k = 4\n",
        "\n",
        "# Simulate words in each document\n",
        "documents = [np.random.choice(vocabulary, num_words, replace=True) for _ in range(num_docs)]\n",
        "\n",
        "# Vocabulary extracted from the documents\n",
        "vocabulary = sorted({word for doc in documents for word in doc})\n",
        "\n",
        "# Create the term-document matrix\n",
        "def create_term_document_matrix(documents, vocabulary):\n",
        "    # Initialize the matrix with zeros\n",
        "    tdm = np.zeros((len(vocabulary), len(documents)), dtype=int)\n",
        "\n",
        "    # Fill the matrix with term frequencies\n",
        "    for j, doc in enumerate(documents):\n",
        "        for term in doc:\n",
        "            i = vocabulary.index(term)\n",
        "            tdm[i, j] += 1\n",
        "\n",
        "    return tdm\n",
        "\n",
        "term_document_matrix = create_term_document_matrix(documents, vocabulary)\n",
        "print(\"Term-Document Matrix:\\n\", pd.DataFrame(term_document_matrix, index=vocabulary, columns=[f\"Doc {i+1}\" for i in range(len(documents))]))\n"
      ],
      "metadata": {
        "id": "mgBw20SObRFy"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "##Singular Value Decomposition (SVD)\n",
        "We will use SVD to decompose the term-document matrix into three matrices\n",
        "$$\n",
        "A = U \\Sigma V^T\n",
        "$$\n",
        "\n",
        "$U$ (left singular vectors) represents the term-topic relationships.\n",
        "\n",
        "$ \\Sigma$ (singular values) represents the strength of each topic.\n",
        "\n",
        "$V^T$ (right singular vectors) represents the document-topic relationships.\n"
      ],
      "metadata": {
        "id": "O1VAya0zbh1G"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "### Simple Example\n",
        "\n",
        "\\\n",
        "A = \\begin{bmatrix}\n",
        "52 & 93 & 15 & 72 \\\\\n",
        "61 & 21 & 83 & 87 \\\\\n",
        "75 & 75 & 88 & 24 \\\\\n",
        "\\end{bmatrix}\n",
        "\n",
        "where\n",
        "$$\n",
        "A = U \\Sigma V^T\n",
        "$$\n",
        "\n",
        "\n",
        "\\\n",
        "U = \\begin{bmatrix}\n",
        "0.534 & 0.798 & -0.279 \\\\\n",
        "0.583 & -0.587 & -0.562 \\\\\n",
        "0.612 & -0.137 & 0.779 \\\\\n",
        "\\end{bmatrix} \\\n",
        "\n",
        "$Î£$ = \\begin{bmatrix}\n",
        "215.673 & 0 & 0 & 0 \\\\\n",
        "0 & 71.245 & 0 & 0 \\\\\n",
        "0 & 0 & 57.976 & 0 \\\\\n",
        "\\end{bmatrix} \\\n",
        "\n",
        "$V^T$ = \\begin{bmatrix}\n",
        "0.507 & 0.500 & 0.511 & 0.482 \\\\\n",
        "-0.065 & 0.724 & -0.685 & 0.044 \\\\\n",
        "0.166 & 0.356 & 0.305 & -0.868 \\\\\n",
        "\\end{bmatrix} \\"
      ],
      "metadata": {
        "id": "VUdrV0DDun3d"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "from numpy.linalg import svd\n",
        "\n",
        "# Apply SVD\n",
        "U, sigma, VT = svd(term_document_matrix, full_matrices=False)\n",
        "\n",
        "# Convert sigma to a diagonal matrix\n",
        "Sigma = np.diag(sigma)\n",
        "\n",
        "print(\"U (Term-Topic Matrix):\\n\", pd.DataFrame(U, index=vocabulary))\n",
        "print(\"\\nSigma (Strength of Topics):\\n\", Sigma)\n",
        "print(\"\\nV^T (Document-Topic Matrix):\\n\", pd.DataFrame(VT, columns=[f\"Doc {i+1}\" for i in range(len(documents))]))\n"
      ],
      "metadata": {
        "id": "1QvPjJmtbpoa"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "##Interpret the Results\n",
        "The matrices from the SVD give us insights into how the terms and documents relate to the underlying topics:\n",
        "\n",
        "\n",
        "$U$ matrix: Each row shows a term's association with different topics. The terms that strongly influence a topic will have higher absolute values in the corresponding column.\n",
        "\n",
        "\\$Î£$ matrix: The diagonal values indicate the importance or strength of each topic. A higher value means the topic is more significant in capturing the structure of the data.\n",
        "\n",
        "\\$V^T$  matrix: Each column shows how much a document contributes to or is represented by each topic.\n",
        "\n",
        "  \n",
        "These results help illustrate latent topics in the dataset by showing which terms group together under topics and how documents relate to these topics. By reducing the dimensionality (selecting top k singular values and corresponding vectors), we can focus on the most meaningful topics in the data."
      ],
      "metadata": {
        "id": "hjMUW_84b23T"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "## Visualize the results"
      ],
      "metadata": {
        "id": "c4D-Bcchc4Tc"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "import seaborn as sns\n",
        "import pandas as pd\n",
        "\n",
        "# Term-Topic Matrix Heatmap\n",
        "plt.figure(figsize=(10, 8))\n",
        "sns.heatmap(pd.DataFrame(U, index=vocabulary), annot=True, fmt=\".2f\", cmap='coolwarm')\n",
        "plt.title('Term-Topic Matrix (U)')\n",
        "plt.xlabel('Topic Index')\n",
        "plt.ylabel('Terms')\n",
        "plt.show()\n",
        "\n",
        "# Document-Topic Matrix Heatmap\n",
        "plt.figure(figsize=(10, 4))\n",
        "sns.heatmap(pd.DataFrame(VT, columns=[f\"Doc {i+1}\" for i in range(len(documents))]), annot=True, fmt=\".2f\", cmap='coolwarm')\n",
        "plt.title('Document-Topic Matrix (V^T)')\n",
        "plt.xlabel('Document Index')\n",
        "plt.ylabel('Topic Index')\n",
        "plt.show()\n",
        "\n"
      ],
      "metadata": {
        "id": "Akd0gbzS-2QI"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "### Dominant Topic for Each Term Using Absolute Values\n",
        "* We will identify the dominant topic for each term based on the highest absolute contribution\n",
        "* Visualize each term with a color corresponding to its dominant topic"
      ],
      "metadata": {
        "id": "K37en-qLpK-u"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "import numpy as np\n",
        "import pandas as pd\n",
        "import matplotlib.pyplot as plt\n",
        "import seaborn as sns\n",
        "\n",
        "# Assuming 'U' is your term-topic matrix from the SVD:\n",
        "term_topic_matrix = pd.DataFrame(U, index=vocabulary)\n",
        "\n",
        "# Calculate absolute values to focus only on the strength of contribution\n",
        "abs_term_topic_matrix = term_topic_matrix.abs()\n",
        "\n",
        "# Find the index of the topic with the highest absolute contribution for each term\n",
        "dominant_topic = np.argmax(abs_term_topic_matrix.values, axis=1)\n",
        "\n",
        "# Create a new DataFrame for visualization\n",
        "terms_dominant_topic = pd.DataFrame({\n",
        "    'Term': term_topic_matrix.index,\n",
        "    'Dominant Topic': dominant_topic,\n",
        "    'Absolute Contribution': [abs_term_topic_matrix.iloc[i, dominant_topic[i]] for i in range(len(dominant_topic))]\n",
        "})\n",
        "\n",
        "# Sort by Dominant Topic\n",
        "terms_dominant_topic.sort_values(by='Dominant Topic', inplace=True)\n",
        "\n"
      ],
      "metadata": {
        "id": "WpGPX_MV4w3P"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Create a color palette with a distinct color for each topic\n",
        "num_topics = len(sigma)  # Assuming 'sigma' holds singular values from the SVD\n",
        "topic_colors = sns.color_palette(\"hsv\", num_topics)\n",
        "\n",
        "# Map colors to topics\n",
        "terms_dominant_topic['Color'] = terms_dominant_topic['Dominant Topic'].apply(lambda x: topic_colors[x])\n",
        "\n",
        "# Plot\n",
        "plt.figure(figsize=(12, 8))\n",
        "plt.barh(terms_dominant_topic['Term'], terms_dominant_topic['Absolute Contribution'], color=terms_dominant_topic['Color'])\n",
        "plt.xlabel('Absolute Contribution to Topic')\n",
        "plt.ylabel('Terms')\n",
        "plt.title('Dominant Topic for Each Term Based on Absolute Contributions')\n",
        "plt.show()\n",
        "\n"
      ],
      "metadata": {
        "id": "01RW7obR4zzG"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "# Non-negative Matrix Factorization (NMF)\n",
        "\n",
        "$V$ is factorized into (usually) two matrices\n",
        "$W$ and $H$, with the property that all three matrices have no negative elements.\n",
        "\n",
        "Given a non-negative matrix $V$, NMF finds two non-negative matrices $W$ and $H$ such that\n",
        "\n",
        "$$V â‰ˆ ð‘Šâ‹…ð»$$\n",
        "\n",
        "The matrix\n",
        "$V$ is usually of size $ð‘šÃ—ð‘›$, where $ð‘š$ is the number of documents and\n",
        "$n$ is the number of terms. $W$ is of size ð‘šÃ—ð‘˜, and $H$ is $kÃ—n$, where\n",
        "ð‘˜ is the number of latent features or topics.\n",
        "\n",
        "The matrix $W$ represents the weight of each topic (latent feature) for each document, and $H$ represents the composition of each feature in terms of the latent features. In the context of topic modeling, $V$ is the term-document frequency matrix, $W$ indicates how much of each topic is present in each document, and $H$ shows how much of each word is present in each topic.\n",
        "\n"
      ],
      "metadata": {
        "id": "S6pktVmsDD5D"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "from sklearn.decomposition import NMF\n",
        "from sklearn.feature_extraction.text import CountVectorizer\n",
        "\n",
        "# Documents\n",
        "documents = [\n",
        "    'paw paw on its its the',\n",
        "    'chased sun mat sat and sat',\n",
        "    'sat sun sat cat the paw',\n",
        "    'mat ball barked chased the ball'\n",
        "]\n",
        "\n",
        "# Step 1: Preprocessing\n",
        "# Create a CountVectorizer, considering the words provided are already preprocessed and stop-words are removed\n",
        "vectorizer = CountVectorizer()\n",
        "X = vectorizer.fit_transform(documents)\n",
        "\n",
        "# Step 2: Applying NMF\n",
        "# Create and fit the NMF model\n",
        "# The number of topics (n_components) is assumed to be 2 for demonstration\n",
        "# This could be determined using model selection techniques or domain knowledge\n",
        "n_components = 3\n",
        "nmf_model = NMF(n_components=n_components, random_state=42)\n",
        "W = nmf_model.fit_transform(X)  # Document-topic matrix\n",
        "H = nmf_model.components_       # Topic-term matrix"
      ],
      "metadata": {
        "id": "yE-e0F7cIK3b"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "W"
      ],
      "metadata": {
        "id": "IGaNrde4IRQa"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# To print the H matrix with terms shown in rows, we can create a DataFrame with terms as the index\n",
        "\n",
        "# Create a DataFrame for H matrix (Topic-Term matrix)\n",
        "H_df = pd.DataFrame(H, columns=feature_names)\n",
        "\n",
        "# Transpose the DataFrame to have terms as rows\n",
        "H_transposed = H_df.T\n",
        "\n",
        "# Print the transposed H matrix with terms as rows\n",
        "print(H_transposed)\n",
        "\n"
      ],
      "metadata": {
        "id": "O8TRUlKvIuhS"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "**Algorithm**: The factorization is achieved through iterative optimization algorithms, such as gradient descent, alternating least squares, or multiplicative update rules. The goal is to minimize the difference between\n",
        "$V$ and the product $Wâ‹…H$, subject to $W$ and $H$ being non-negative.NMF is not unique, and different initializations can lead to different results."
      ],
      "metadata": {
        "id": "m86At0KPJw46"
      }
    }
  ]
}