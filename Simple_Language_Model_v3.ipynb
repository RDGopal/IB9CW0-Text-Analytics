{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "provenance": []
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "cells": [
    {
      "cell_type": "markdown",
      "source": [
        "#Simple Language Model with Transformer\n",
        "We will now explore using a simple version of a Transformer model from PyTorch, a popular deep learning framework.\n",
        "\n"
      ],
      "metadata": {
        "id": "n23wsJQeQ15f"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "#Preliminaries"
      ],
      "metadata": {
        "id": "9phfRfjQV9cd"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "import nltk\n",
        "import torch\n",
        "import torch.nn as nn\n",
        "import torch.optim as optim\n",
        "import numpy as np\n",
        "from gensim.models import Word2Vec\n",
        "from nltk.tokenize import word_tokenize\n",
        "nltk.download('punkt')\n",
        "import requests\n",
        "\n",
        "# URL to the raw text file on GitHub\n",
        "url = 'https://raw.githubusercontent.com/RDGopal/IB9CW0-Text-Analytics/main/Data/tinyshakespeare.txt'\n",
        "\n",
        "# Use requests to get the content of the file\n",
        "response = requests.get(url)\n",
        "\n",
        "# Ensure the request was successful\n",
        "if response.status_code == 200:\n",
        "    s_text = response.text\n",
        "    # Continue processing the text as needed\n",
        "else:\n",
        "    print(\"Failed to retrieve the file. Status code:\", response.status_code)\n",
        "\n",
        "# Tokenize the text\n",
        "tokens = word_tokenize(s_text)\n",
        "\n",
        "# Organize the tokens into sentences, Word2Vec needs data in the format of list of lists of tokens\n",
        "sentences = [tokens[i:i+100] for i in range(0, len(tokens), 100)]\n",
        "\n",
        "# Train the CBOW model\n",
        "word2vec_model = Word2Vec(sentences, vector_size=5, window=5, min_count=1, sg=0)"
      ],
      "metadata": {
        "id": "Kc7pBNfKQ28e"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Generate training data for the GPT model\n",
        "def generate_training_data(tokens, model_wv, context_size):\n",
        "    X, y = [], []\n",
        "    sequence = [word for word in tokens if word in model_wv.key_to_index]\n",
        "    for i in range(len(sequence) - context_size):\n",
        "        context_words = sequence[i:i + context_size]\n",
        "        target_word = sequence[i + context_size]\n",
        "        context_embeddings = [model_wv[word] for word in context_words]\n",
        "        target_embedding = model_wv[target_word]\n",
        "        X.append(context_embeddings)\n",
        "        y.append(target_embedding)\n",
        "    return np.array(X), np.array(y)\n",
        "\n",
        "context_size = 5\n",
        "X, y = generate_training_data(tokens, word2vec_model.wv, context_size)\n",
        "X_train_tensor = torch.tensor(X, dtype=torch.float32)\n",
        "y_train_tensor = torch.tensor(y, dtype=torch.float32)"
      ],
      "metadata": {
        "id": "t-twkLSURFgC"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "#GPT Model"
      ],
      "metadata": {
        "id": "V2IxPzSiWBnQ"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "The structure of the model is as follows:\n",
        "\n",
        "![picture](https://drive.google.com/uc?export=view&id=1o45KgGRYze9_G7jI65q4vA5PKTBsGyzc)\n"
      ],
      "metadata": {
        "id": "uUvP8UxaI41k"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "Key Elements of the Model are the following.\n",
        "\n",
        "**Position Embeddings:**\n",
        "\n",
        "Adds position information to the embeddings, crucial for maintaining sequence order understanding within the transformer.\n",
        "\n",
        "**Transformer Encoder:**\n",
        "\n",
        "Processes the entire input sequence with self-attention and feed-forward layers, allowing the model to understand and utilize contextual information from the entire sequence.\n",
        "\n",
        "**Output Layer:**\n",
        "\n",
        "Maps the transformer's output (specifically from the last token) back to the embedding dimension. This output is intended to be a prediction of the embedding of the next word."
      ],
      "metadata": {
        "id": "SfElhB1Shwno"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "import torch\n",
        "import torch.nn as nn\n",
        "import torch.nn.functional as F\n",
        "\n",
        "class EmbeddingPredictorGPT(nn.Module):\n",
        "    def __init__(self, embed_dim, num_heads, num_layers, max_seq_length):\n",
        "        super(EmbeddingPredictorGPT, self).__init__()\n",
        "        self.embed_dim = embed_dim\n",
        "        self.max_seq_length = max_seq_length\n",
        "        self.position_embeddings = nn.Parameter(torch.zeros(1, max_seq_length, embed_dim))\n",
        "\n",
        "        # Transformer Encoder Layer\n",
        "        encoder_layer = nn.TransformerEncoderLayer(\n",
        "            d_model=embed_dim, nhead=num_heads, dim_feedforward=embed_dim * 4, batch_first=True)\n",
        "        self.transformer = nn.TransformerEncoder(encoder_layer, num_layers=num_layers)\n",
        "\n",
        "        # Output layer that predicts the embedding of the next word\n",
        "        self.output_layer = nn.Linear(embed_dim, embed_dim)\n",
        "\n",
        "    def forward(self, x):\n",
        "        # x is expected to be of shape (batch_size, seq_length, embed_dim)\n",
        "        # Add position embeddings\n",
        "        seq_length = x.size(1)\n",
        "        position_encoded = x + self.position_embeddings[:, :seq_length, :]\n",
        "\n",
        "        # Passing through the transformer encoder\n",
        "        transformed = self.transformer(position_encoded)\n",
        "\n",
        "        # Using the output from the last token to predict the next embedding\n",
        "        prediction = self.output_layer(transformed[:, -1, :])\n",
        "        return prediction\n"
      ],
      "metadata": {
        "id": "3bPlGGmFhv75"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "#Loss Function\n",
        "\n",
        "We will construct a loss function based on cosine similarity between the predicted embedding and the target embedding. The loss is defined as the 1 - cosine similarity. Note that the loss value will range from 0 to 2. Closer to 0 the final trained model loss, better is the quality of learning from the training data.\n",
        "\n",
        "**The training takes a long while to complete.**\n",
        "\n",
        "*I have saved the trained model as `language_model_3.pth`. You can load the trained to run inference (i.e. next word prediction and text generation).*"
      ],
      "metadata": {
        "id": "XUCfkP9PiHbd"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "import torch\n",
        "import torch.nn as nn\n",
        "import torch.optim as optim\n",
        "\n",
        "model = EmbeddingPredictorGPT(embed_dim=5, num_heads=1, num_layers=3, max_seq_length=5)\n",
        "\n",
        "# Define cosine similarity and criterion as a loss\n",
        "cosine_similarity = nn.CosineSimilarity(dim=1)\n",
        "\n",
        "def criterion(output, target):\n",
        "    # Subtracting from 1 converts similarity to dissimilarity (1 means identical, 0 means orthogonal)\n",
        "    loss = 1 - cosine_similarity(output, target)\n",
        "    return loss.mean()  # Ensure scalar output by taking mean across all data\n",
        "\n",
        "optimizer = optim.Adam(model.parameters(), lr=0.001)\n",
        "\n",
        "# Training loop\n",
        "for epoch in range(200):  # epochs\n",
        "    optimizer.zero_grad()\n",
        "    outputs = model(X_train_tensor)\n",
        "    loss = criterion(outputs, y_train_tensor)\n",
        "    print(f\"Outputs shape: {outputs.shape}, Loss value: {loss}\")  # Debugging output shapes and loss\n",
        "    loss.backward()\n",
        "    optimizer.step()\n",
        "\n",
        "    if epoch % 10 == 0:\n",
        "        print(f'Epoch {epoch}, Loss: {loss.item()}')\n",
        "\n",
        "\n",
        "# Save the entire model\n",
        "torch.save(model, 'language_model_3.pth')\n"
      ],
      "metadata": {
        "id": "lVXNfJ2RNvyg"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "Load the trained model as follows."
      ],
      "metadata": {
        "id": "Y02UEjdOa3_i"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# Later you can load it with\n",
        "gpt_model = torch.load('language_model_3.pth')\n",
        "gpt_model.eval()  # Set the model to evaluation mode if you're loading for inference"
      ],
      "metadata": {
        "id": "v1v0QvG0hvbJ"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "Compute the loss value for the prediction model on the training data"
      ],
      "metadata": {
        "id": "DRpwj0GDlZGX"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "import torch\n",
        "import torch.nn as nn\n",
        "\n",
        "# Define the cosine similarity function\n",
        "cosine_similarity = nn.CosineSimilarity(dim=1)\n",
        "\n",
        "# Custom cosine dissimilarity function for calculating loss\n",
        "def cosine_dissimilarity(output, target):\n",
        "    # Calculate the cosine dissimilarity\n",
        "    return 1 - cosine_similarity(output, target)\n",
        "\n",
        "# No need to track gradients here as we're only evaluating\n",
        "with torch.no_grad():\n",
        "    # Run the model on the training data\n",
        "    training_outputs = gpt_model(X_train_tensor)\n",
        "\n",
        "    # Compute the cosine dissimilarity between the outputs and the actual targets\n",
        "    training_loss = cosine_dissimilarity(training_outputs, y_train_tensor).mean()  # Ensure it's a scalar by taking the mean\n",
        "\n",
        "    # Print the computed loss\n",
        "    print(f'Training Loss: {training_loss.item()}')\n"
      ],
      "metadata": {
        "id": "zuo7cM27Xv8s"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "# Number of parameters\n",
        "The following code computes the number of parameters that are estimated in out model.\n"
      ],
      "metadata": {
        "id": "OevvReLFmoWi"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "def count_trainable_parameters(model):\n",
        "    trainable_params = sum(p.numel() for p in model.parameters() if p.requires_grad)\n",
        "    return trainable_params\n",
        "\n",
        "trainable_parameters = count_trainable_parameters(gpt_model)\n",
        "print(f'Number of trainable parameters: {trainable_parameters}')"
      ],
      "metadata": {
        "id": "0Gyp-qoMlUQV"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "#Next Word Prediction\n",
        "* Word2Vec Model (word2vec_model): This model is trained on the sentences extracted from the text and used to generate embeddings.\n",
        "\n",
        "* SimpleGPT Model (gpt_model): This is the transformer-based model used for training on sequences of embeddings to predict the next word's embedding.\n",
        "\n",
        "* Predict Next Words Function: This function takes an input text, processes it through the trained GPT model (gpt_model), and uses the Word2Vec model (word2vec_model.wv) to find the closest words to the predicted embedding."
      ],
      "metadata": {
        "id": "FdbW2llonjnz"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "def predict_next_words(input_text, model, model_wv, num_predictions=5):\n",
        "    tokens = word_tokenize(input_text)\n",
        "    last_words = tokens[-5:]  # Get the last 5 words from the input text\n",
        "    last_words = [word for word in last_words if word in model_wv.key_to_index]  # Filter out words not in model vocabulary\n",
        "    input_embeddings = np.array([model_wv[word] for word in last_words])\n",
        "    input_tensor = torch.tensor(input_embeddings, dtype=torch.float32).unsqueeze(0)  # Convert to tensor and add batch dimension\n",
        "\n",
        "    model.eval()\n",
        "    with torch.no_grad():\n",
        "        output_embeddings = model(input_tensor)\n",
        "        predicted_embedding = output_embeddings[0]\n",
        "\n",
        "    # Get all embeddings from the Word2Vec model\n",
        "    all_embeddings = torch.tensor([model_wv[word] for word in model_wv.index_to_key], dtype=torch.float32)\n",
        "    cos = torch.nn.CosineSimilarity(dim=1)\n",
        "    similarities = cos(predicted_embedding.unsqueeze(0), all_embeddings)\n",
        "    top_indices = similarities.topk(num_predictions).indices\n",
        "    closest_words = [model_wv.index_to_key[idx] for idx in top_indices]\n",
        "    return closest_words\n"
      ],
      "metadata": {
        "id": "JDdmHe13o30W"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Example usage\n",
        "input_text = \"to be or not\" # @param {type:\"string\"}\n",
        "predicted_words = predict_next_words(input_text, gpt_model, word2vec_model.wv)"
      ],
      "metadata": {
        "id": "ak6FJFV4niwG"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "predicted_words"
      ],
      "metadata": {
        "id": "JQkrW6tLnikO"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "#Text Generation\n",
        "* Temperature: This parameter controls how much to weigh the probabilities. A lower temperature makes the model more confident (less random), while a higher temperature makes the choices softer, increasing diversity.\n",
        "\n",
        "* Probabilistic Choice: Instead of selecting the word with the highest cosine similarity, the function now converts similarities into probabilities (using softmax) and samples from these probabilities, which introduces randomness into the word selection process."
      ],
      "metadata": {
        "id": "-wlcP9K9pXWh"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "import torch\n",
        "import torch.nn.functional as F\n",
        "import numpy as np\n",
        "\n",
        "def generate_text(model, model_wv, starting_sequence, num_words=5, temperature=1.0):\n",
        "    \"\"\"\n",
        "    Generate text using the trained GPT model and Word2Vec embeddings with added randomness.\n",
        "\n",
        "    Args:\n",
        "        model (torch.nn.Module): The trained GPT model.\n",
        "        model_wv (gensim.models.keyedvectors.KeyedVectors): Word2Vec model's embeddings.\n",
        "        starting_sequence (str): The initial text sequence to start text generation.\n",
        "        num_words (int): Number of words to generate.\n",
        "        temperature (float): Temperature parameter to control randomness of predictions.\n",
        "\n",
        "    Returns:\n",
        "        str: The generated text.\n",
        "    \"\"\"\n",
        "    model.eval()  # Set the model to evaluation mode\n",
        "    text = starting_sequence\n",
        "    words = starting_sequence.split()\n",
        "\n",
        "    for _ in range(num_words):\n",
        "        last_words = words[-5:]  # Only use the last 5 words or fewer if available\n",
        "        input_embeddings = np.array([model_wv[word] for word in last_words if word in model_wv.key_to_index])\n",
        "        input_tensor = torch.tensor(input_embeddings, dtype=torch.float32).unsqueeze(0)  # Add batch dimension\n",
        "\n",
        "        # Predict the next word's embedding\n",
        "        with torch.no_grad():\n",
        "            output_embeddings = model(input_tensor)  # Removed positions argument\n",
        "            predicted_embedding = output_embeddings[0]\n",
        "\n",
        "        # Find the closest word in the Word2Vec vocabulary\n",
        "        all_embeddings = torch.tensor([model_wv[word] for word in model_wv.index_to_key], dtype=torch.float32)\n",
        "        cos = torch.nn.CosineSimilarity(dim=1)\n",
        "        similarities = cos(predicted_embedding.unsqueeze(0), all_embeddings)\n",
        "\n",
        "        # Apply softmax to get probabilities\n",
        "        probabilities = F.softmax(similarities / temperature, dim=0)\n",
        "        next_word_idx = torch.multinomial(probabilities, 1).item()\n",
        "        next_word = model_wv.index_to_key[next_word_idx]\n",
        "\n",
        "        # Append the predicted word to the text\n",
        "        text += ' ' + next_word\n",
        "        words.append(next_word)\n",
        "\n",
        "    return text"
      ],
      "metadata": {
        "id": "y0d0FBhknie4"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Example usage\n",
        "starting_sequence = \"to be or not to\" # @param {type:\"string\"}\n",
        "num_generated_words = 20\n",
        "# Temperature - Lower for more conservative predictions, higher for more diversity\n",
        "temperature = 0.48  # @param {type:\"slider\", min:0, max:1, step:0.01}\n",
        "alpha = 0.5\n",
        "\n",
        "generated_text = generate_text(gpt_model, word2vec_model.wv, starting_sequence, num_generated_words, temperature)"
      ],
      "metadata": {
        "id": "4HnBcixjpreg"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "generated_text"
      ],
      "metadata": {
        "id": "8gu24MVUqf_V"
      },
      "execution_count": null,
      "outputs": []
    }
  ]
}