{
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/RDGopal/IB9CW0-Text-Analytics/blob/main/Lecture3.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "uE8bZiPYuFd-"
      },
      "source": [
        "# Topic Modeling\n",
        "Topic Modeling is a type of statistical modeling for discovering the abstract \"topics\" that occur in a collection of documents. It is a frequent technique in text mining for uncovering hidden semantic structures in a text body. This process is useful in various applications like organizing large archives of texts, summarizing information, and aiding in understanding the main themes of texts without reading them in full. We will look at several techniques for topic modeling including LDA, LSA, NMF and BERTopic.\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "25yLd5ocYXUo"
      },
      "source": [
        "## LDA"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "95lCXgr3wqu1"
      },
      "outputs": [],
      "source": [
        "import nltk\n",
        "from nltk.corpus import stopwords\n",
        "from nltk.stem import WordNetLemmatizer, PorterStemmer, SnowballStemmer"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "QAqJytSaw2ON"
      },
      "outputs": [],
      "source": [
        "nltk.download('punkt')\n",
        "nltk.download('stopwords')\n",
        "nltk.download('wordnet')\n",
        "# nltk.download() to look at options"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "pQhxTxC-w_gl"
      },
      "outputs": [],
      "source": [
        "import gensim\n",
        "from gensim.corpora import Dictionary\n",
        "from gensim.models import LdaModel"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "_dZIXC-il2WQ"
      },
      "outputs": [],
      "source": [
        "# Function to preprocess the data\n",
        "def preprocess_texts(documents):\n",
        "    stop_words = set(stopwords.words('english'))\n",
        "    lemmatizer = WordNetLemmatizer()\n",
        "\n",
        "    preprocessed_texts = []\n",
        "    for text in documents:\n",
        "        text = text.lower()\n",
        "        tokens = nltk.word_tokenize(text)\n",
        "        tokens = [token for token in tokens if token not in stop_words and token.isalpha()]\n",
        "        tokens = [lemmatizer.lemmatize(token) for token in tokens]\n",
        "        preprocessed_texts.append(tokens)  # Append list of tokens directly, without joining them into a string\n",
        "\n",
        "    return preprocessed_texts\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "gunkoo8EuAg5"
      },
      "outputs": [],
      "source": [
        "# Sample data and preprocessing\n",
        "documents = [\n",
        "    \"Sugar is bad to consume. My sister likes to have sugar, but not my father.\",\n",
        "    \"My father spends a lot of time driving my brother around to baseball practice.\",\n",
        "    \"Some health experts suggest that driving may cause increased stress and blood pressure.\",\n",
        "    \"I often feel pressure to perform well at school, but my father never seems to drive my sister to do better.\",\n",
        "    \"Health professionals say that Sugar is not good for your lifestyle.\"\n",
        "]\n",
        "\n",
        "# 'texts' will be a list of lists of tokens\n",
        "texts = preprocess_texts(documents)\n",
        "\n",
        "dictionary = Dictionary(texts)\n",
        "\n",
        "# Create a document-term matrix\n",
        "corpus = [dictionary.doc2bow(text) for text in texts]\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "QXI8X2EU8GlT"
      },
      "outputs": [],
      "source": [
        "# Set parameters\n",
        "num_topics = 2\n",
        "passes = 10\n",
        "\n",
        "# Create the LDA model\n",
        "lda = LdaModel(corpus=corpus, id2word=dictionary, num_topics=num_topics, passes=passes)\n",
        "\n",
        "# Display the topics\n",
        "topics = lda.print_topics(num_words=5)\n",
        "for topic in topics:\n",
        "    print(topic)\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "8QCbB1c389rT"
      },
      "source": [
        "The variations in output each time you run an LDA model are due to the stochastic nature of the algorithms used in Latent Dirichlet Allocation. Here are the key reasons for these variations:\n",
        "\n",
        "* Random Initialization: Most LDA implementations (including gensim) start with a random initialization of the topic assignments for each word. This randomness affects the subsequent updates and iterations of the algorithm.\n",
        "\n",
        "* Sampling Methods: LDA often uses sampling methods like Gibbs sampling or variational Bayes inference to estimate the distributions of topics over words and documents. These methods inherently involve randomness, which can lead to different results each time the model is trained, especially if the number of iterations is not large enough to reach convergence.\n",
        "\n",
        "* Convergence Issues: If the LDA model doesn't fully converge due to too few iterations or poor parameter settings (like learning rates), the output may vary significantly between runs. Achieving convergence in probabilistic models can be challenging and might require tuning parameters such as the number of iterations and choosing appropriate hyperparameters (like alpha and beta for LDA).\n",
        "\n",
        "* Number of Topics: The choice of the number of topics (num_topics) can also influence the stability of your topics. Too few or too many topics can lead to overfitting or underfitting, respectively, making the model sensitive to initial conditions or specific samples of data.\n",
        "\n",
        "### Ensuring Consistency\n",
        "\n",
        "To reduce the variability in your LDA models, you can take the following steps:\n",
        "\n",
        "* Set a Random Seed: Most LDA implementations allow you to set a random seed to make the results reproducible. This is done by fixing the seed for the random number generator used by the model.\n",
        "\n",
        "* Increase the Number of Iterations: Allowing more iterations can help the model converge more consistently, which, in turn, makes the output more stable.\n",
        "\n",
        "* Tune Hyperparameters: Adjusting the hyperparameters, such as alpha (document-topic density) and beta (topic-word density), can help in achieving better and more consistent results. These parameters control the sparsity of the topics extracted by the model.\n",
        "\n",
        "* Extensive Preprocessing: More consistent and thorough preprocessing of the text data can lead to less noise in the input, which helps stabilize the output.\n",
        "\n",
        "```\n",
        "lda_model = LdaModel(\n",
        "    corpus=corpus,\n",
        "    id2word=dictionary,\n",
        "    num_topics=10,\n",
        "    random_state=100,\n",
        "    update_every=1,\n",
        "    chunksize=100,\n",
        "    passes=10,\n",
        "    alpha='auto',  # can be set as a scalar like '0.01' or 'auto' for learning the optimal alpha\n",
        "    eta='auto'     # eta is another name for beta in gensim, similarly settable\n",
        ")\n",
        "```\n",
        "\n",
        "\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "Fs40G5UJ9EAe"
      },
      "outputs": [],
      "source": [
        "# Set parameters\n",
        "num_topics = 3\n",
        "passes = 100\n",
        "\n",
        "# Create the LDA model\n",
        "lda = LdaModel(corpus=corpus, id2word=dictionary, num_topics=num_topics, passes=passes,random_state=100)\n",
        "\n",
        "# Display the topics\n",
        "topics = lda.print_topics(num_words=5)\n",
        "for topic in topics:\n",
        "    print(topic)\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "gGUq2f7xbu3S"
      },
      "source": [
        "We can also display the topics as wordclouds:"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "cDpnJt_ndc5m"
      },
      "outputs": [],
      "source": [
        "%pip install wordcloud"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "JarpWoA-b605"
      },
      "outputs": [],
      "source": [
        "from wordcloud import WordCloud\n",
        "import matplotlib.pyplot as plt\n",
        "\n",
        "def create_wordcloud(model, topic):\n",
        "    text = {word: value for word, value in model.show_topic(topic, 200)}\n",
        "    wc = WordCloud(background_color=\"white\", max_words=1000)\n",
        "    wc.generate_from_frequencies(text)\n",
        "    plt.imshow(wc, interpolation=\"bilinear\")\n",
        "    plt.axis(\"off\")\n",
        "    return plt\n",
        "\n",
        "create_wordcloud(lda, topic=0)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "1NMB6kB9Eh3y"
      },
      "source": [
        "#Coherence Score\n",
        "Coherence measures the degree of semantic similarity between high scoring words in the topic. These scores help distinguish between topics that are semantically interpretable topics and topics that are artifacts of statistical inference."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "K4YgR5cYEq8-"
      },
      "outputs": [],
      "source": [
        "from gensim.models.coherencemodel import CoherenceModel\n",
        "coherence_model_lda = CoherenceModel(model=lda, texts=texts, dictionary=dictionary, coherence='c_v')\n",
        "coherence_lda = coherence_model_lda.get_coherence()\n",
        "print('Coherence Score:', coherence_lda)\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "VZtrrFSXFM2D"
      },
      "source": [
        "# Number of Topics"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "Hb9s1BoZFQSZ"
      },
      "outputs": [],
      "source": [
        "import matplotlib.pyplot as plt\n",
        "\n",
        "# Range of topics to evaluate\n",
        "topic_range = range(2, 6)\n",
        "\n",
        "# List to store coherence\n",
        "coherence_scores = []\n",
        "\n",
        "for num_topics in topic_range:\n",
        "    lda = LdaModel(corpus=corpus, id2word=dictionary, num_topics=num_topics, passes=passes, random_state=100)\n",
        "    # Initialize CoherenceModel after training the LDA model\n",
        "    coherence_model_lda = CoherenceModel(model=lda, texts=texts, dictionary=dictionary, coherence='c_v')\n",
        "    coherence_lda = coherence_model_lda.get_coherence()\n",
        "\n",
        "    # Append the coherence score\n",
        "    coherence_scores.append(coherence_lda)\n",
        "\n",
        "print(coherence_scores)\n",
        "# Plotting\n",
        "plt.figure(figsize=(12, 6))\n",
        "plt.plot(topic_range, coherence_scores)\n",
        "plt.title(\"Coherence Scores vs Number of Topics\")\n",
        "plt.xlabel(\"Number of Topics\")\n",
        "plt.ylabel(\"Coherence score\")\n",
        "plt.tight_layout()\n",
        "plt.show()\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "9Mn104ldIGUF"
      },
      "source": [
        "# LSA"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "IE1nBgWAUYHM"
      },
      "outputs": [],
      "source": [
        "from gensim.models import LdaModel,CoherenceModel,TfidfModel,Nmf,LsiModel"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "gRO7BenfTg_S"
      },
      "outputs": [],
      "source": [
        "# train LSA model\n",
        "num_topics = 4\n",
        "lsa_model = LsiModel(corpus, num_topics=num_topics, id2word=dictionary,random_seed=100)\n",
        "topics = lsa_model.print_topics(num_words=5)\n",
        "for topic in topics:\n",
        "    print(topic)\n",
        "\n",
        "coherence_model_lsa = CoherenceModel(model=lsa_model, texts=texts, dictionary=dictionary, coherence='c_v')\n",
        "coherence_lsa = coherence_model_lsa.get_coherence()\n",
        "print('Coherence Score:', coherence_lsa)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "CGO6fN4DZL7w"
      },
      "source": [
        "##NMF"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "o36mwfwrUs2q"
      },
      "outputs": [],
      "source": [
        "# train NMF model\n",
        "nmf_model = Nmf(corpus,id2word=dictionary, num_topics=num_topics,passes=20,random_state=100)\n",
        "topics = nmf_model.print_topics(num_words=5)\n",
        "for topic in topics:\n",
        "    print(topic)\n",
        "\n",
        "coherence_model_nmf = CoherenceModel(model=nmf_model, texts=texts, dictionary=dictionary, coherence='c_v')\n",
        "coherence_nmf = coherence_model_nmf.get_coherence()\n",
        "print('Coherence Score:', coherence_nmf)\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "nTn7UfZvZOYf"
      },
      "source": [
        "##LSA vs NMF\n",
        "* Comparison and Use Cases Interpretability: NMF often provides more interpretable topics than LSA because of the non-negativity constraint, which aligns better with the intuitive notion of adding up features to form a dataset.\n",
        "* Handling of Synonyms and Polysemy: LSA can handle synonyms better due to its dimensional reduction approach, but it struggles with polysemy (words with multiple meanings) because all occurrences of a word are mapped to a single point in semantic space.\n",
        "* Usage: LSA is often preferred for tasks involving similarity and information retrieval due to its robust handling of noise and ability to capture underlying semantic patterns. NMF is favored for clustering and classification tasks where interpretability of components (topics) is crucial.\n",
        "\n",
        " Both LSA and NMF are valuable tools in the text mining and information retrieval toolkit, chosen based on the specific requirements of clarity, ease of interpretation, and the nature of the text data."
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "PLfkWvyMy-Wa"
      },
      "source": [
        "## BERT Topic Modeling\n",
        "BERTopic is a topic modeling tool that leverages transformers and language models like BERT for generating topics from textual data. Unlike traditional methods such as LDA, BERTopic utilizes contextual embeddings, which can capture the meanings of words more effectively in different contexts.\n",
        "\n",
        "Since the example we have used thus far is too small for this technique (due to embeddings - we will discuss this later), we will use the first 100 reviews from the ``Reviews.csv`` file."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "background_save": true
        },
        "id": "8YLRjZ833e8B"
      },
      "outputs": [],
      "source": [
        "%pip install bertopic"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "background_save": true
        },
        "id": "Wk84c-g2H8TO"
      },
      "outputs": [],
      "source": [
        "import nltk\n",
        "import pandas as pd\n",
        "from nltk.corpus import stopwords\n",
        "from nltk.tokenize import word_tokenize\n",
        "\n",
        "# Download necessary NLTK resources\n",
        "nltk.download('punkt')\n",
        "nltk.download('stopwords')"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "fVOLxvF770DI"
      },
      "outputs": [],
      "source": [
        "import pandas as pd\n",
        "\n",
        "# Load the dataset and read the first 100 rows\n",
        "#file_path = 'Reviews.csv'\n",
        "#df = pd.read_csv(file_path,nrows=100)\n",
        "\n",
        "df = pd.read_csv('https://raw.githubusercontent.com/RDGopal/IB9CW0-Text-Analytics/main/Data/Reviews.csv',nrows=100)\n",
        "\n",
        "# Display the first few rows of the dataset and columns to identify the 'Review' column\n",
        "df.head(), df.columns\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "background_save": true
        },
        "id": "aK4Wuq1cHYPe"
      },
      "outputs": [],
      "source": [
        "# Put it all into a function\n",
        "def preprocess_text(text):\n",
        "    # Lowercase\n",
        "    text = text.lower()\n",
        "    # Tokenize\n",
        "    tokens = nltk.word_tokenize(text)\n",
        "    # Remove stopwords\n",
        "    stop_words = set(stopwords.words('english'))\n",
        "    tokens = [token for token in tokens if token not in stop_words and token.isalpha()]\n",
        "    return ' '.join(tokens)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "background_save": true
        },
        "id": "5eT8BBgTIN1O"
      },
      "outputs": [],
      "source": [
        "# Remove rows where 'Review' is NaN\n",
        "df = df.dropna(subset=['Review'])\n",
        "df['Review']= df['Review'].apply(preprocess_text)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "KQ1cM-NtKNum"
      },
      "source": [
        "### BERTopic"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "background_save": true
        },
        "id": "SW8rNKEwAvK4"
      },
      "outputs": [],
      "source": [
        "from bertopic import BERTopic"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "background_save": true
        },
        "id": "E3m0Kqd-8Elg"
      },
      "outputs": [],
      "source": [
        "# Initialize BERTopic\n",
        "topic_model = BERTopic(language=\"english\", calculate_probabilities=True)\n",
        "\n",
        "# Fit the model on the limited data\n",
        "topics, probabilities = topic_model.fit_transform(df['Review'])\n",
        "\n",
        "# Display the topic info\n",
        "topic_info = topic_model.get_topic_info()\n",
        "print(topic_info)\n",
        "\n",
        "# Optionally, print individual topics\n",
        "for topic in topic_model.get_topics().items():\n",
        "    print(topic)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "159lFcvKKTTu"
      },
      "source": [
        "### Visualize"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "background_save": true
        },
        "id": "U82l9O13BBeI"
      },
      "outputs": [],
      "source": [
        "import matplotlib.pyplot as plt\n",
        "import numpy as np\n",
        "\n",
        "# Assuming 'topics' contains the output from BERTopic\n",
        "unique_topics, counts = np.unique(topics, return_counts=True)\n",
        "\n",
        "# Simple bar chart to show topic distribution\n",
        "plt.bar(unique_topics, counts, color='skyblue')\n",
        "plt.xlabel('Topic Number')\n",
        "plt.ylabel('Number of Documents')\n",
        "plt.title('Topic Distribution')\n",
        "plt.xticks(unique_topics)\n",
        "plt.show()\n",
        "\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "n3t72sS5KYTW"
      },
      "source": [
        "### Top terms for each topic"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "background_save": true
        },
        "id": "-FZ0FJwQC6Xn"
      },
      "outputs": [],
      "source": [
        "data = topic_model.get_topics()\n",
        "rows = []\n",
        "for topic, terms in data.items():\n",
        "    for term, weight in terms:\n",
        "        rows.append({'topic': topic, 'term': term, 'weight': weight})\n",
        "df = pd.DataFrame(rows)\n",
        "sorted_df = df.sort_values(by=['topic', 'weight'], ascending=[True, False])\n",
        "print(sorted_df)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "71COunlbSclg"
      },
      "source": [
        "# Some practical considerations"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "vXxZZY6DSf4Y"
      },
      "outputs": [],
      "source": [
        "# Load the dataset and read the first 100 rows\n",
        "#file_path = 'Reviews.csv'\n",
        "#df = pd.read_csv(file_path,nrows=100)\n",
        "df = pd.read_csv('https://raw.githubusercontent.com/RDGopal/IB9CW0-Text-Analytics/main/Data/Reviews.csv',nrows=100)\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "uPFV6PeJSv5m"
      },
      "source": [
        "## Preprocessing\n",
        "Depending on the context, you may want to focus on terms that have a particular part-of-speech. For this case, let's only keep terms that are nouns and adjectives. Here are the steps to modify the `preprocess_text` function:\n",
        "\n",
        "* Tokenize the text.\n",
        "* Use part-of-speech tagging to identify the POS of each token.\n",
        "* Filter out tokens that are not nouns or adjectives.\n",
        "* Remove stopwords and non-alphabetic tokens.\n",
        "* Join the tokens back into a string."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "pAf_2YSYSwUo"
      },
      "outputs": [],
      "source": [
        "import nltk\n",
        "from nltk.corpus import stopwords\n",
        "from nltk.tokenize import word_tokenize\n",
        "from nltk import pos_tag\n",
        "\n",
        "nltk.download('averaged_perceptron_tagger')\n",
        "\n",
        "def preprocess_text(text):\n",
        "    # Lowercase\n",
        "    text = text.lower()\n",
        "\n",
        "    # Tokenize\n",
        "    tokens = nltk.word_tokenize(text)\n",
        "\n",
        "    # Part-of-speech tagging\n",
        "    tagged_tokens = nltk.pos_tag(tokens)\n",
        "\n",
        "    # Filter tokens that are nouns or adjectives\n",
        "    # NN* for nouns, JJ* for adjectives\n",
        "    tokens = [word for word, tag in tagged_tokens if tag in ['NN', 'NNS', 'NNP', 'NNPS', 'JJ', 'JJR', 'JJS']]\n",
        "\n",
        "    # Remove stopwords\n",
        "    stop_words = set(stopwords.words('english'))\n",
        "    tokens = [token for token in tokens if token not in stop_words and token.isalpha()]\n",
        "\n",
        "    return ' '.join(tokens)\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "mvcM-T1iS3D4"
      },
      "outputs": [],
      "source": [
        "# Remove rows where 'Review' is NaN\n",
        "df = df.dropna(subset=['Review'])\n",
        "df['Review']= df['Review'].apply(preprocess_text)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "hmqIelpiTBOY"
      },
      "outputs": [],
      "source": [
        "df[['Review']]"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "QIdBFK8ETTJg"
      },
      "outputs": [],
      "source": [
        "# 'texts' will be a list of lists of tokens\n",
        "texts = preprocess_texts(df['Review'])\n",
        "\n",
        "dictionary = Dictionary(texts)\n",
        "\n",
        "# Create a document-term matrix\n",
        "corpus = [dictionary.doc2bow(text) for text in texts]"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "GhDzy62AYFaX"
      },
      "source": [
        "## LDA"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "N5TWdzogTaXo"
      },
      "outputs": [],
      "source": [
        "# Set parameters\n",
        "num_topics = 2\n",
        "passes = 10\n",
        "\n",
        "# Create the LDA model\n",
        "lda = LdaModel(corpus=corpus, id2word=dictionary, num_topics=num_topics, passes=passes)\n",
        "\n",
        "# Display the topics\n",
        "topics = lda.print_topics(num_words=10)\n",
        "for topic in topics:\n",
        "    print(topic)\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "M_64FJi2YJdv"
      },
      "source": [
        "## Choose the number of topics"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "NeAfflQ7T5UA"
      },
      "outputs": [],
      "source": [
        "import matplotlib.pyplot as plt\n",
        "\n",
        "# Range of topics to evaluate\n",
        "topic_range = range(2, 6)\n",
        "\n",
        "# List to store coherence\n",
        "coherence_scores = []\n",
        "\n",
        "for num_topics in topic_range:\n",
        "    lda = LdaModel(corpus=corpus, id2word=dictionary, num_topics=num_topics, passes=passes, random_state=100)\n",
        "    # Initialize CoherenceModel after training the LDA model\n",
        "    coherence_model_lda = CoherenceModel(model=lda, texts=texts, dictionary=dictionary, coherence='c_v')\n",
        "    coherence_lda = coherence_model_lda.get_coherence()\n",
        "\n",
        "    # Append the coherence score\n",
        "    coherence_scores.append(coherence_lda)\n",
        "\n",
        "print(coherence_scores)\n",
        "# Plotting\n",
        "plt.figure(figsize=(12, 6))\n",
        "plt.plot(topic_range, coherence_scores)\n",
        "plt.title(\"Coherence Scores vs Number of Topics\")\n",
        "plt.xlabel(\"Number of Topics\")\n",
        "plt.ylabel(\"Coherence score\")\n",
        "plt.tight_layout()\n",
        "plt.show()"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "t6LYb8oFUbJ4"
      },
      "outputs": [],
      "source": [
        "# Set optimal number of topics\n",
        "num_topics = 4\n",
        "passes = 10\n",
        "\n",
        "# Create the LDA model\n",
        "lda = LdaModel(corpus=corpus, id2word=dictionary, num_topics=num_topics, passes=passes)\n",
        "\n",
        "# Display the topics\n",
        "topics = lda.print_topics(num_words=10)\n",
        "for topic in topics:\n",
        "    print(topic)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "vStwGjpNVXYW"
      },
      "source": [
        "## Visualize"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "vOCx3jdyWspe"
      },
      "source": [
        "### Print top 5 terms in each topic"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "9DUJZCKJWgRW"
      },
      "outputs": [],
      "source": [
        "# Print the top 5 terms for each topic\n",
        "for topic_id in range(num_topics):\n",
        "    top_terms = lda.show_topic(topic_id, topn=5)  # Get the top 5 terms for this topic\n",
        "    terms = ', '.join([term for term, _ in top_terms])\n",
        "    print(f\"Topic {topic_id}: {terms}\")"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "_xk_K_zpWwq3"
      },
      "source": [
        "## Interactive Web-based Visualization"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "rIKw-02BVZif"
      },
      "outputs": [],
      "source": [
        "%pip install pyLDAvis"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "3WzkBgvwVfHA"
      },
      "outputs": [],
      "source": [
        "import pyLDAvis\n",
        "import pyLDAvis.gensim_models as gensimvis\n",
        "\n",
        "# Assume `lda` is the LDA model and `corpus` and `dictionary` are your corpus and dictionary from Gensim\n",
        "\n",
        "# Prepare the visualization data\n",
        "vis_data = gensimvis.prepare(lda, corpus, dictionary)\n",
        "\n",
        "# Display the visualization in a Jupyter Notebook (or in an IPython environment)\n",
        "pyLDAvis.display(vis_data)\n",
        "\n",
        "# To save the visualization as an HTML file\n",
        "pyLDAvis.save_html(vis_data, 'lda_visualization.html')\n"
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "# Your Turn\n",
        "Conduct topic modeling with `sms_spam.csv`and `oct_delta.csv` data."
      ],
      "metadata": {
        "id": "SyuHy2e3PkQw"
      }
    }
  ],
  "metadata": {
    "colab": {
      "provenance": [],
      "include_colab_link": true
    },
    "kernelspec": {
      "display_name": "Python 3",
      "name": "python3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "nbformat": 4,
  "nbformat_minor": 0
}