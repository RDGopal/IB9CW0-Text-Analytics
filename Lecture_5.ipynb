{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "provenance": [],
      "authorship_tag": "ABX9TyN9b9dvvzRaHcxMPIhonuNL",
      "include_colab_link": true
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/RDGopal/IB9CW0-Text-Analytics/blob/main/Lecture_5.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "# Supervised Learning with Text\n",
        "In this session, we will demonstrate how to perform predictive modeling with text data. We will use the `sms_spam.csv` dataset to analyze and predict whether an SMS is spam or not. We will explore three different approaches to achieve this:\n",
        "1.\tCreate word tokens and use TF-IDF values to capture the essence of the text, and use these to predict the outcome.\n",
        "2. Compute sentiment of the text, and use the sentiment captured in the text to predict the outcome.\n",
        "3.\tUse topic modeling to identify k topics, and use the percentage of each topic captured in the text to predict the outcome.\n",
        "\n",
        "In a later session, we will also explore using word embeddings for predictive purposes.\n"
      ],
      "metadata": {
        "id": "wdJFVjmHTwwz"
      }
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "m7F1_Bm_TtyT"
      },
      "outputs": [],
      "source": [
        "import pandas as pd\n",
        "import re\n",
        "import nltk\n",
        "\n",
        "from nltk.corpus import stopwords\n",
        "nltk.download('punkt')\n",
        "nltk.download('stopwords')\n",
        "nltk.download('wordnet')"
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "## Read the file"
      ],
      "metadata": {
        "id": "FtAHq0XHUj3D"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "from google.colab import drive\n",
        "drive.mount('/content/drive')"
      ],
      "metadata": {
        "id": "Bo33HP7TUoF6"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# upload files using file picker\n",
        "from google.colab import files\n",
        "files.upload()"
      ],
      "metadata": {
        "id": "eOd7jSrJVBni"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "df = pd.read_csv('https://raw.githubusercontent.com/RDGopal/IB9CW0-Text-Analytics/main/Data/sms_spam.csv')"
      ],
      "metadata": {
        "id": "QYmEFXUqVowC"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "df"
      ],
      "metadata": {
        "id": "RiKwjhXjVsJq"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "## Preprocess"
      ],
      "metadata": {
        "id": "G-sRVUq9V7mx"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# Put it all into a function\n",
        "def preprocess_text(text):\n",
        "    # Lowercase\n",
        "    text = text.lower()\n",
        "    # Tokenize\n",
        "    tokens = nltk.word_tokenize(text)\n",
        "    # Remove stopwords\n",
        "    stop_words = set(stopwords.words('english'))\n",
        "    tokens = [token for token in tokens if token not in stop_words and token.isalpha()]\n",
        "    return ' '.join(tokens)\n",
        "\n"
      ],
      "metadata": {
        "id": "pBpIqHrgV-VS"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "df['text'] = df['text'].apply(preprocess_text)"
      ],
      "metadata": {
        "id": "9Nj-mbzoWHGR"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "df"
      ],
      "metadata": {
        "id": "jEkG862uTz-y"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "##TF-IDF\n",
        "We will use the 100 most important terms, write them out to the file, and build a prediction model using random forest algorithm."
      ],
      "metadata": {
        "id": "-LvIv6J8MfN5"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "from sklearn.feature_extraction.text import TfidfVectorizer"
      ],
      "metadata": {
        "id": "dr5FPd09W-JB"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "##Compute tf-idf and predict based on these"
      ],
      "metadata": {
        "id": "6tuDUYj2WlXl"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "from sklearn.ensemble import RandomForestClassifier\n",
        "from sklearn.model_selection import train_test_split\n",
        "from sklearn.metrics import confusion_matrix, roc_curve, auc, classification_report\n",
        "import matplotlib.pyplot as plt\n",
        "import numpy as np\n",
        "\n",
        "\n",
        "# Prepare the data using the correct TF-IDF features\n",
        "vectorizer_corrected = TfidfVectorizer(max_features=100)\n",
        "tfidf_matrix_corrected = vectorizer_corrected.fit_transform(df['text'])\n",
        "tfidf_df_corrected = pd.DataFrame(tfidf_matrix_corrected.toarray(), columns=vectorizer_corrected.get_feature_names_out())\n",
        "df_corrected = pd.concat([df.drop(columns=['text']), tfidf_df_corrected], axis=1)  # Exclude original 'text' column for modeling\n",
        "\n",
        "# Prepare the data for modeling\n",
        "X = df_corrected.drop(columns=['type'])  # Features\n",
        "y = df_corrected['type']  # Target\n",
        "\n",
        "# Split data into training and testing sets\n",
        "X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.2, random_state=42)\n",
        "\n",
        "# Fill NaN values with zeros (common practice in TF-IDF where NaN signifies no occurrence of the term)\n",
        "X_train = X_train.fillna(0)\n",
        "X_test = X_test.fillna(0)\n",
        "\n",
        "\n",
        "# Initialize and train the Random Forest classifier\n",
        "rf_classifier = RandomForestClassifier(n_estimators=100, random_state=42)\n",
        "rf_classifier.fit(X_train, y_train)\n",
        "\n",
        "# Predict on the test set\n",
        "y_pred = rf_classifier.predict(X_test)\n",
        "y_prob = rf_classifier.predict_proba(X_test)[:, 1]  # probabilities for ROC curve\n",
        "\n",
        "# Confusion Matrix\n",
        "conf_matrix = confusion_matrix(y_test, y_pred)\n",
        "print(\"Confusion Matrix:\\n\", conf_matrix)\n",
        "\n",
        "# ROC Curve\n",
        "fpr, tpr, _ = roc_curve(y_test, y_prob,pos_label='spam')\n",
        "roc_auc = auc(fpr, tpr)\n",
        "\n",
        "plt.figure()\n",
        "plt.plot(fpr, tpr, color='darkorange', lw=2, label='ROC curve (area = %0.2f)' % roc_auc)\n",
        "plt.plot([0, 1], [0, 1], color='navy', lw=2, linestyle='--')\n",
        "plt.xlim([0.0, 1.0])\n",
        "plt.ylim([0.0, 1.05])\n",
        "plt.xlabel('False Positive Rate')\n",
        "plt.ylabel('True Positive Rate')\n",
        "plt.title('Receiver Operating Characteristic')\n",
        "plt.legend(loc=\"lower right\")\n",
        "plt.show()\n",
        "\n",
        "# Feature Importance\n",
        "importances = rf_classifier.feature_importances_\n",
        "indices = np.argsort(importances)[::-1]\n",
        "print(\"Feature ranking:\")\n",
        "\n",
        "for f in range(X_train.shape[1]):\n",
        "    print(\"%d. embedding %d (%f)\" % (f + 1, indices[f], importances[indices[f]]))\n",
        "\n"
      ],
      "metadata": {
        "id": "mXsGswjVdOSp"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "## VADER Sentiment\n",
        "Compute VADER sentiment and write it to the data frame."
      ],
      "metadata": {
        "id": "COARIt1UptR2"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "import nltk\n",
        "from nltk.sentiment import SentimentIntensityAnalyzer\n",
        "\n",
        "# Download VADER lexicon if not already downloaded\n",
        "nltk.download('vader_lexicon')\n",
        "\n",
        "# Initialize the VADER sentiment analyzer\n",
        "sia = SentimentIntensityAnalyzer()\n",
        "\n",
        "# Function to get the compound sentiment score for each text\n",
        "def get_sentiment(text):\n",
        "    return sia.polarity_scores(text)['compound']\n",
        "\n",
        "# Apply the function to get sentiment score for each message in the DataFrame\n",
        "df['vader_sentiment'] = df['text'].apply(get_sentiment)\n",
        "\n",
        "# Display the first few rows to verify sentiment scores\n",
        "df[['text', 'vader_sentiment']].head()\n"
      ],
      "metadata": {
        "id": "J6Hz-mEgpxY-"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "### Prediction Model"
      ],
      "metadata": {
        "id": "O6F2DYDtNYRJ"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "from sklearn.model_selection import train_test_split\n",
        "from sklearn.ensemble import RandomForestClassifier\n",
        "from sklearn.metrics import classification_report\n",
        "\n",
        "# Example setup: Assume 'vader_sentiment' column exists in df\n",
        "# Create sample sentiment data for demonstration (normally you would have real scores here)\n",
        "import numpy as np\n",
        "df['vader_sentiment'] = np.random.rand(df.shape[0])  # Random scores for illustration\n",
        "\n",
        "# Prepare the data for modeling\n",
        "X = df[['vader_sentiment']]  # Predictor\n",
        "y= df['type']  # Target\n",
        "\n",
        "# Split the data into training and test sets\n",
        "X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.2, random_state=42)\n",
        "\n",
        "# Initialize and train the Random Forest classifier\n",
        "rf_classifier = RandomForestClassifier(n_estimators=100, random_state=42)\n",
        "rf_classifier.fit(X_train, y_train)\n",
        "\n",
        "# Predict on the test set\n",
        "y_pred = rf_classifier.predict(X_test)\n",
        "y_prob = rf_classifier.predict_proba(X_test)[:, 1]  # probabilities for ROC curve\n",
        "\n",
        "# Confusion Matrix\n",
        "conf_matrix = confusion_matrix(y_test, y_pred)\n",
        "print(\"Confusion Matrix:\\n\", conf_matrix)\n",
        "\n",
        "# ROC Curve\n",
        "fpr, tpr, _ = roc_curve(y_test, y_prob,pos_label='spam')\n",
        "roc_auc = auc(fpr, tpr)\n",
        "\n",
        "plt.figure()\n",
        "plt.plot(fpr, tpr, color='darkorange', lw=2, label='ROC curve (area = %0.2f)' % roc_auc)\n",
        "plt.plot([0, 1], [0, 1], color='navy', lw=2, linestyle='--')\n",
        "plt.xlim([0.0, 1.0])\n",
        "plt.ylim([0.0, 1.05])\n",
        "plt.xlabel('False Positive Rate')\n",
        "plt.ylabel('True Positive Rate')\n",
        "plt.title('Receiver Operating Characteristic')\n",
        "plt.legend(loc=\"lower right\")\n",
        "plt.show()\n",
        "\n",
        "# Feature Importance\n",
        "importances = rf_classifier.feature_importances_\n",
        "indices = np.argsort(importances)[::-1]\n",
        "print(\"Feature ranking:\")\n",
        "\n",
        "for f in range(X_train.shape[1]):\n",
        "    print(\"%d. embedding %d (%f)\" % (f + 1, indices[f], importances[indices[f]]))\n"
      ],
      "metadata": {
        "id": "i8pqATX_p0OH"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "## Topic Modeling\n",
        "We use LDA to create 10 topics and write them out to the data frame."
      ],
      "metadata": {
        "id": "K1Lefch1Hfiq"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "import gensim\n",
        "import gensim.corpora as corpora\n",
        "from gensim.models.ldamodel import LdaModel"
      ],
      "metadata": {
        "id": "ulhCOWc7KCCp"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "###LDA"
      ],
      "metadata": {
        "id": "GkDUVcRgN1-R"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# Assuming the text is preprocessed and stored as a list of words in df['text_preprocessed']\n",
        "# If df['text'] is a single string of words, split it into lists\n",
        "if isinstance(df['text'].iloc[0], str):\n",
        "    df['text_preprocessed'] = df['text'].apply(lambda x: x.split())\n",
        "\n",
        "# Create a Dictionary and Corpus from the preprocessed text\n",
        "id2word_preprocessed = corpora.Dictionary(df['text_preprocessed'])\n",
        "corpus_preprocessed = [id2word_preprocessed.doc2bow(text) for text in df['text_preprocessed']]\n",
        "\n",
        "# Step 2: Initialize and fit the LDA model\n",
        "lda_model_preprocessed = LdaModel(corpus=corpus_preprocessed, id2word=id2word_preprocessed, num_topics=10, random_state=42, update_every=1, passes=10, alpha='auto', per_word_topics=True)\n",
        "\n",
        "# Step 3: Extract Topic Distributions\n",
        "topic_distributions_preprocessed = [lda_model_preprocessed.get_document_topics(bow) for bow in corpus_preprocessed]\n",
        "\n",
        "# Normalize topic distributions and ensure each document has distribution over all 10 topics\n",
        "def normalize_topic_distributions(distributions, num_topics=10):\n",
        "    normalized = []\n",
        "    for dist in distributions:\n",
        "        doc_topics = dict(dist)\n",
        "        normalized.append([doc_topics.get(i, 0) for i in range(num_topics)])\n",
        "    return normalized\n",
        "\n",
        "# Normalize topic distributions\n",
        "normalized_topics_preprocessed = normalize_topic_distributions(topic_distributions_preprocessed)\n",
        "\n",
        "# Create DataFrame with topic distributions\n",
        "topics_df_preprocessed = pd.DataFrame(normalized_topics_preprocessed, columns=[f\"Topic{i+1}\" for i in range(10)])\n",
        "\n",
        "# Concatenate original DataFrame with topics DataFrame\n",
        "df_with_topics_gensim_preprocessed = pd.concat([df, topics_df_preprocessed], axis=1)\n",
        "\n",
        "# Display the DataFrame with topics\n",
        "df_with_topics_gensim_preprocessed.head()\n"
      ],
      "metadata": {
        "id": "L9BZDqUeKEeQ"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "###Predict and Compute Performance Metrics"
      ],
      "metadata": {
        "id": "uYap-IPlN7LY"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# Prepare the data for modeling, excluding one topic for independence\n",
        "X = df_with_topics_gensim_preprocessed[[f\"Topic{i+1}\" for i in range(9)]]  # Using Topic1 to Topic9\n",
        "y = df_with_topics_gensim_preprocessed['type']  # Target variable\n",
        "\n",
        "# Split the data into training and test sets\n",
        "X_train, X_test, y_train, y_test = train_test_split(\n",
        "    X, y, test_size=0.2, random_state=42)\n",
        "\n",
        "# Initialize and train the Random Forest classifier\n",
        "rf_classifier = RandomForestClassifier(n_estimators=100, random_state=42)\n",
        "rf_classifier.fit(X_train, y_train)\n",
        "\n",
        "# Predict on the test set\n",
        "y_pred = rf_classifier.predict(X_test)\n",
        "\n",
        "# Calculate and print the confusion matrix\n",
        "conf_matrix = confusion_matrix(y_test, y_pred)\n",
        "print(\"Confusion Matrix:\\n\", conf_matrix)\n",
        "\n",
        "# Calculate ROC curve and AUC if both classes are present in the test set\n",
        "if len(np.unique(y_test)) > 1:\n",
        "    y_prob = rf_classifier.predict_proba(X_test)[:, 1]\n",
        "    fpr, tpr, _ = roc_curve(y_test, y_prob, pos_label='spam')\n",
        "    roc_auc = auc(fpr, tpr)\n",
        "\n",
        "    # Plot ROC curve\n",
        "    plt.figure()\n",
        "    plt.plot(fpr, tpr, color='darkorange', lw=2, label='ROC curve (area = %0.2f)' % roc_auc)\n",
        "    plt.plot([0, 1], [0, 1], color='navy', lw=2, linestyle='--')\n",
        "    plt.xlim([0.0, 1.0])\n",
        "    plt.ylim([0.0, 1.05])\n",
        "    plt.xlabel('False Positive Rate')\n",
        "    plt.ylabel('True Positive Rate')\n",
        "    plt.title('Receiver Operating Characteristic')\n",
        "    plt.legend(loc=\"lower right\")\n",
        "    plt.show()\n",
        "else:\n",
        "    print(\"ROC curve not plotted. The test set does not contain both classes.\")\n",
        "\n",
        "# Feature importances from the Random Forest model\n",
        "feature_importances = pd.DataFrame({\n",
        "    'Feature': X.columns,\n",
        "    'Importance': rf_classifier.feature_importances_\n",
        "}).sort_values(by='Importance', ascending=False)\n",
        "\n",
        "feature_importances\n"
      ],
      "metadata": {
        "id": "XvGdDmF6LcKp"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "# Your Turn\n",
        "1.\t Read the `Roomba.csv` file. Create a new variable called `rating` which has values `high` and `low`. If the `Stars` value is 4 or 5, then `rating` should be `high`, otherwise `low`. Build models to predict `rating` based on `Review`. Conduct topic modeling (10 topics) and sentiments of `Review` and use these to predict  `rating`.\n",
        "\n",
        "2.\tRead the `imdb.csv` file and predict the sentiment based on the review of the movie.\n"
      ],
      "metadata": {
        "id": "kD_mf--4XdsQ"
      }
    }
  ]
}