{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "provenance": [],
      "mount_file_id": "19_Rizx3bZ1InfdRfFVdBc3m12kVkLdmu",
      "authorship_tag": "ABX9TyMwvzBiQ9PpFj4RUckzpDqA",
      "include_colab_link": true
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/RDGopal/IB9CW0-Text-Analytics/blob/main/Lecture2.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "# Text Processing"
      ],
      "metadata": {
        "id": "mcDV8lrlsb_Y"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "import pandas as pd\n",
        "import re\n",
        "import nltk"
      ],
      "metadata": {
        "id": "eTsD-Rfxs8Cf"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "from nltk.corpus import stopwords\n",
        "from nltk.stem import WordNetLemmatizer, PorterStemmer, SnowballStemmer"
      ],
      "metadata": {
        "id": "jfLT5lbXtF1H"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "# Download extra packages"
      ],
      "metadata": {
        "id": "pCRPyADTtSb1"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "nltk.download('punkt')\n",
        "nltk.download('stopwords')\n",
        "nltk.download('wordnet')"
      ],
      "metadata": {
        "id": "XYprK53utM0_"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "# Read the file"
      ],
      "metadata": {
        "id": "22gQuGdMzvjE"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "df = pd.read_csv('https://raw.githubusercontent.com/RDGopal/IB9CW0-Text-Analytics/main/Data/sms_spam.csv')"
      ],
      "metadata": {
        "id": "ysli7eIXzy4e"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "df"
      ],
      "metadata": {
        "id": "TOLK7hEF1FoN"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "# Accessing files from Github"
      ],
      "metadata": {
        "id": "tJYBZX-nBS08"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "import requests\n",
        "\n",
        "def list_github_directory(user, repo, path):\n",
        "    url = f\"https://api.github.com/repos/{user}/{repo}/contents/{path}\"\n",
        "    response = requests.get(url)\n",
        "    if response.status_code == 200:\n",
        "        content = response.json()\n",
        "        return [file['name'] for file in content if file['type'] == 'file']\n",
        "    else:\n",
        "        print(\"Failed to retrieve data:\", response.status_code)\n",
        "        return []\n",
        "\n",
        "# Usage\n",
        "user = 'RDGopal'\n",
        "repo = 'IB9CW0-Text-Analytics'\n",
        "path = 'Data'\n",
        "files = list_github_directory(user, repo, path)\n",
        "print(\"Files in the Data folder:\", files)\n"
      ],
      "metadata": {
        "id": "NnoVbw6BBXfz"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "# Data Preprocessing"
      ],
      "metadata": {
        "id": "nHGy52Uv1ljt"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# Lowercase\n",
        "df['text'] = df['text'].str.lower()"
      ],
      "metadata": {
        "id": "qKvtHACE1qlN"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Tokenize\n",
        "df['tokens'] = df['text'].apply(nltk.word_tokenize)"
      ],
      "metadata": {
        "id": "lzn7m_R15Sqt"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# get stopwords\n",
        "stop_words = stopwords.words('english')\n",
        "extra_words = ['.','*',',']\n",
        "stop_words.extend(extra_words)"
      ],
      "metadata": {
        "id": "hen-yoKZ5pkF"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Remove stopwords\n",
        "df['tokens'] = df['tokens'].apply(lambda tokens: [token for token in tokens if token not in stop_words and token.isalpha()])"
      ],
      "metadata": {
        "id": "CiMxr6Bu-NT1"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Create a PorterStemmer object\n",
        "stemmer = PorterStemmer()\n",
        "df['tokens'] = df['tokens'].apply(lambda tokens: [stemmer.stem(token) for token in tokens])"
      ],
      "metadata": {
        "id": "9dGi1FVz-tTV"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Lemmatize\n",
        "lemmatizer = WordNetLemmatizer()\n",
        "df['tokens'] = df['tokens'].apply(lambda tokens: [lemmatizer.lemmatize(token) for token in tokens])\n"
      ],
      "metadata": {
        "id": "Lfz9iht_-891"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Combine tokens back into a cleaned review\n",
        "df['text1'] = df['tokens'].apply(lambda tokens: ' '.join(tokens))"
      ],
      "metadata": {
        "id": "3ssLzou3_Yx1"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "df[['text','text1']]"
      ],
      "metadata": {
        "id": "4yFdrmFB_rwT"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Put it all into a function\n",
        "def preprocess_text(text):\n",
        "    # Lowercase\n",
        "    text = text.lower()\n",
        "    # Tokenize\n",
        "    tokens = nltk.word_tokenize(text)\n",
        "    # Remove stopwords\n",
        "    stop_words = stopwords.words('english')\n",
        "    extra_words = ['.','*',',']\n",
        "    stop_words.extend(extra_words)\n",
        "    tokens = [token for token in tokens if token not in stop_words and token.isalpha()]\n",
        "    # Lemmatize\n",
        "    lemmatizer = WordNetLemmatizer()\n",
        "    tokens = [lemmatizer.lemmatize(token) for token in tokens]\n",
        "    return ' '.join(tokens)"
      ],
      "metadata": {
        "id": "-ca8XWqCEzb1"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "\n",
        "df['text'] = df['text'].apply(preprocess_text)"
      ],
      "metadata": {
        "id": "-aicK2tZEbLU"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "#  Your Turn\n",
        "Read and preprocess the file `oct_delta.csv`"
      ],
      "metadata": {
        "id": "aGGA0DpUHRGM"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "# Bag-of-Words and tf-idf"
      ],
      "metadata": {
        "id": "cJez0UcIFf5a"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "import gensim\n",
        "from gensim.corpora import Dictionary\n",
        "from gensim.models import LdaModel,CoherenceModel,TfidfModel,Nmf,LsiModel"
      ],
      "metadata": {
        "id": "POsQsoJ4Fp3l"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# create list of tokens\n",
        "documents = df['tokens'].tolist()"
      ],
      "metadata": {
        "id": "WWoTnFRgFsAS"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "documents"
      ],
      "metadata": {
        "id": "784Z59uMF4Qy"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# create the dictionary\n",
        "dictionary = Dictionary(documents) # list of lists (documents)"
      ],
      "metadata": {
        "id": "7_amKNStGJs0"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Optional if want to see the dictionary\n",
        "dictionary.save_as_text('testxyz.csv',sort_by_word=True)"
      ],
      "metadata": {
        "id": "VWcvS4nZGLfz"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# filter extremes from the dictionary (optional, but recommended)\n",
        "dictionary.filter_extremes(no_below=5, no_above=0.5) # at least appears in 5 documents, no more than 50%"
      ],
      "metadata": {
        "id": "zSN_KP9PHx8j"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# create bag-of-words representation\n",
        "corpus = [dictionary.doc2bow(document) for document in documents]"
      ],
      "metadata": {
        "id": "Ac1RV8wVH9_z"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "corpus"
      ],
      "metadata": {
        "id": "CtoHhe3rID_C"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# create tf-idf representation\n",
        "tfidf_model = TfidfModel(corpus)\n",
        "tfidf_corpus = [tfidf_model[doc] for doc in corpus]"
      ],
      "metadata": {
        "id": "P7bpSUzcISSU"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "tfidf_corpus"
      ],
      "metadata": {
        "id": "itbQf-3DIW07"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "# Frequency Analysis"
      ],
      "metadata": {
        "id": "lFGhVIaYx87X"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "!pip install matplotlib wordcloud\n"
      ],
      "metadata": {
        "id": "18kZkWaWyAI3"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "To perform frequency analysis, we need to count how often each word appears in your corpus. We can utilize the dictionary and the Bag-of-Words (BoW) corpus"
      ],
      "metadata": {
        "id": "J5eyDC9jyTUf"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "from collections import Counter\n",
        "import matplotlib.pyplot as plt\n",
        "\n",
        "# Summing up the counts from the BoW corpus\n",
        "total_word_count = Counter(word_id for document in corpus for word_id, count in document)\n",
        "\n",
        "# Mapping back the word IDs to words\n",
        "mapped_word_counts = [(dictionary[word_id], count) for word_id, count in total_word_count.items()]\n",
        "\n",
        "# Sort words by frequency\n",
        "sorted_word_counts = sorted(mapped_word_counts, key=lambda w: w[1], reverse=True)\n",
        "\n",
        "# Let's plot the top words\n",
        "plt.figure(figsize=(10, 5))\n",
        "words, counts = zip(*sorted_word_counts[:40])\n",
        "plt.bar(words, counts)\n",
        "plt.xlabel('Words')\n",
        "plt.ylabel('Frequency')\n",
        "plt.title('Top 20 Words by Frequency')\n",
        "plt.xticks(rotation=90)\n",
        "plt.show()\n"
      ],
      "metadata": {
        "id": "H8DZx-ZZyNaM"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "# Creating a Word Cloud\n",
        "To create a word cloud, you will need the frequencies in a dictionary format, where the keys are words and the values are their frequencies."
      ],
      "metadata": {
        "id": "JEweScxUy_qm"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "from wordcloud import WordCloud\n",
        "\n",
        "# Creating a dictionary for word cloud\n",
        "word_freq_dict = dict(sorted_word_counts)\n",
        "\n",
        "# Creating word cloud\n",
        "wordcloud = WordCloud(width = 800, height = 400, background_color ='white').generate_from_frequencies(word_freq_dict)\n",
        "\n",
        "# Displaying the word cloud\n",
        "plt.figure(figsize=(15, 8))\n",
        "plt.imshow(wordcloud, interpolation='bilinear')\n",
        "plt.axis('off')  # Turn off axis numbers and ticks\n",
        "plt.show()\n"
      ],
      "metadata": {
        "id": "-t_9AkV_zGhg"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "# Your Turn\n",
        "Conduct frequency analysis with `oct_delta.csv` file."
      ],
      "metadata": {
        "id": "SMxY-w0RKSfy"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "# Tiny Shakespeare novel"
      ],
      "metadata": {
        "id": "xO8hZeCr2fxD"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "### Read the file locally"
      ],
      "metadata": {
        "id": "F_vddh2JFG7Z"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "import nltk\n",
        "from nltk.corpus import stopwords\n",
        "from nltk.tokenize import word_tokenize\n",
        "\n",
        "# Download necessary NLTK resources\n",
        "nltk.download('punkt')\n",
        "nltk.download('stopwords')\n",
        "\n",
        "# Load the text file\n",
        "with open('/content/tinyshakespeare.txt', 'r') as file:\n",
        "    text = file.read().lower()\n",
        "\n",
        "# Tokenize the text\n",
        "tokens = word_tokenize(text)\n"
      ],
      "metadata": {
        "id": "6IKd9lig2os_"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "### Or Read the file from GitHub"
      ],
      "metadata": {
        "id": "GswT33ABFK7I"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "import nltk\n",
        "from nltk.corpus import stopwords\n",
        "from nltk.tokenize import word_tokenize\n",
        "\n",
        "# Download necessary NLTK resources\n",
        "nltk.download('punkt')\n",
        "nltk.download('stopwords')\n",
        "\n",
        "import requests\n",
        "\n",
        "# URL to the raw text file on GitHub\n",
        "url = 'https://raw.githubusercontent.com/RDGopal/IB9CW0-Text-Analytics/main/Data/tinyshakespeare.txt'\n",
        "\n",
        "# Use requests to get the content of the file\n",
        "response = requests.get(url)\n",
        "\n",
        "# Ensure the request was successful\n",
        "if response.status_code == 200:\n",
        "    text = response.text.lower()\n",
        "    # Continue processing the text as needed\n",
        "else:\n",
        "    print(\"Failed to retrieve the file. Status code:\", response.status_code)\n",
        "\n",
        "# Tokenize the text\n",
        "tokens = word_tokenize(text)"
      ],
      "metadata": {
        "id": "48_gKi3iEKpK"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "len(tokens)"
      ],
      "metadata": {
        "id": "xmpZvKm52v1H"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "## Zipf's Law\n",
        "\n",
        "Zipf's Law is an empirical law that suggests the frequency of a word in a natural language text is inversely proportional to its rank in the frequency table. To test Zipf's Law with your tokenized text data, we follow the steps below:\n",
        "\n",
        "1. **Calculate Word Frequencies**: Count how often each word appears in your tokenized text.\n",
        "\n",
        "2. **Sort Words by Frequency**: Rank the words by their frequency in descending order.\n",
        "\n",
        "3. **Plot the Frequencies**: Plot the frequency of each word against its rank on a log-log plot.\n",
        "\n",
        "4. **Analyze the Plot**: Zipf's Law predicts a linear relationship on a log-log plot with a slope of approximately -1.\n"
      ],
      "metadata": {
        "id": "JhkYl1iXGC9R"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "import nltk\n",
        "from nltk.tokenize import word_tokenize\n",
        "import matplotlib.pyplot as plt\n",
        "from collections import Counter\n",
        "import numpy as np\n",
        "\n",
        "# Count frequencies\n",
        "word_counts = Counter(tokens)\n",
        "\n",
        "# Sort words by frequency\n",
        "sorted_word_counts = word_counts.most_common()\n",
        "\n",
        "# Prepare data for plotting\n",
        "ranks = range(1, len(sorted_word_counts) + 1)\n",
        "frequencies = [freq for _, freq in sorted_word_counts]\n",
        "\n",
        "# Log-log plot\n",
        "plt.figure(figsize=(8, 6))\n",
        "plt.loglog(ranks, frequencies, marker=\"o\")\n",
        "plt.title('Zipf\\'s Law')\n",
        "plt.xlabel('Rank of the word')\n",
        "plt.ylabel('Frequency of the word')\n",
        "plt.grid(True)\n",
        "plt.show()\n"
      ],
      "metadata": {
        "id": "wLlP-MXPGHsf"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Get English stopwords\n",
        "stop_words = set(stopwords.words('english'))\n",
        "\n",
        "# Remove stopwords from tokens\n",
        "filtered_tokens = [token for token in tokens if token not in stop_words and token.isalpha()]\n"
      ],
      "metadata": {
        "id": "fsPogqVZ20m2"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "len(filtered_tokens)"
      ],
      "metadata": {
        "id": "BqhMHoTh29fW"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "from collections import Counter\n",
        "\n",
        "# Count word frequencies\n",
        "word_counts = Counter(filtered_tokens)\n"
      ],
      "metadata": {
        "id": "D6PA1qlu3Xnn"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "word_counts"
      ],
      "metadata": {
        "id": "lNK67sFj3aHm"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "from wordcloud import WordCloud\n",
        "import matplotlib.pyplot as plt\n",
        "\n",
        "# Create a word cloud\n",
        "wordcloud = WordCloud(width = 800, height = 400, background_color ='white').generate_from_frequencies(word_counts)\n",
        "\n",
        "# Display the word cloud\n",
        "plt.figure(figsize=(15, 8))\n",
        "plt.imshow(wordcloud, interpolation='bilinear')\n",
        "plt.axis('off')  # Turn off axis numbers and ticks\n",
        "plt.show()\n"
      ],
      "metadata": {
        "id": "f_aFueIs3k_U"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "import matplotlib.pyplot as plt\n",
        "\n",
        "# Get the 20 most common words\n",
        "most_common_words = word_counts.most_common(20)\n",
        "\n",
        "# Unpack the words and their frequencies\n",
        "words, frequencies = zip(*most_common_words)\n",
        "\n",
        "# Create a bar chart\n",
        "plt.figure(figsize=(10, 8))\n",
        "plt.bar(words, frequencies)\n",
        "plt.xlabel('Words')\n",
        "plt.ylabel('Frequency')\n",
        "plt.title('Top 20 Most Frequent Words')\n",
        "plt.xticks(rotation=45)  # Rotate the words on x-axis to avoid overlapping\n",
        "plt.show()\n"
      ],
      "metadata": {
        "id": "eRVInZzs36IT"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "# Your Turn\n",
        "Run the following code to get a novel from HugginFace and conduct Zipf's law analysis."
      ],
      "metadata": {
        "id": "8c0SHY2sKiPZ"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "import nltk\n",
        "from nltk.corpus import stopwords\n",
        "from nltk.tokenize import word_tokenize\n",
        "\n",
        "# Download necessary NLTK resources\n",
        "nltk.download('punkt')\n",
        "nltk.download('stopwords')\n",
        "\n",
        "import requests\n",
        "\n",
        "# URL to the raw text file on GitHub\n",
        "url =\"https://datasets-server.huggingface.co/rows?dataset=JiggaBooJombs%2FNovelist&config=default&split=train&offset=0&length=100\"\n",
        "\n",
        "# Use requests to get the content of the file\n",
        "response = requests.get(url)\n",
        "\n",
        "# Ensure the request was successful\n",
        "if response.status_code == 200:\n",
        "    text = response.text.lower()\n",
        "    # Continue processing the text as needed\n",
        "else:\n",
        "    print(\"Failed to retrieve the file. Status code:\", response.status_code)\n",
        "\n",
        "# Tokenize the text\n",
        "tokens = word_tokenize(text)"
      ],
      "metadata": {
        "id": "JEQIJ_qqKjky"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "# POS Tagging"
      ],
      "metadata": {
        "id": "kooyfJGl1Rck"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# Download necessary NLTK resources\n",
        "nltk.download('punkt')\n",
        "nltk.download('stopwords')\n",
        "nltk.download('averaged_perceptron_tagger')"
      ],
      "metadata": {
        "id": "rDxee82w60Ya"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "\n",
        "\n",
        "# POS tagging the filtered tokens\n",
        "pos_tags = nltk.pos_tag(filtered_tokens)\n",
        "\n",
        "# Count word frequencies (including the POS tags for uniqueness)\n",
        "word_counts = Counter(pos_tags)\n",
        "\n",
        "# Create a list of dictionaries to later convert to a DataFrame\n",
        "data = [{'Word': word, 'POS': pos, 'WordCount': count} for (word, pos), count in word_counts.items()]\n",
        "\n",
        "# Create the DataFrame\n",
        "df_pos = pd.DataFrame(data)\n",
        "\n",
        "# Display the first few rows of the DataFrame to verify\n",
        "print(df_pos.head())"
      ],
      "metadata": {
        "id": "09yMxD0-1TdI"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "## Create POS mapping"
      ],
      "metadata": {
        "id": "s513SrJJE5Az"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "pos_tag_full_form = {\n",
        "    'CC': 'Coordinating conjunction',\n",
        "    'CD': 'Cardinal number',\n",
        "    'DT': 'Determiner',\n",
        "    'EX': 'Existential there',\n",
        "    'FW': 'Foreign word',\n",
        "    'IN': 'Preposition or subordinating conjunction',\n",
        "    'JJ': 'Adjective',\n",
        "    'JJR': 'Adjective, comparative',\n",
        "    'JJS': 'Adjective, superlative',\n",
        "    'LS': 'List item marker',\n",
        "    'MD': 'Modal',\n",
        "    'NN': 'Noun, singular or mass',\n",
        "    'NNS': 'Noun, plural',\n",
        "    'NNP': 'Proper noun, singular',\n",
        "    'NNPS': 'Proper noun, plural',\n",
        "    'PDT': 'Predeterminer',\n",
        "    'POS': 'Possessive ending',\n",
        "    'PRP': 'Personal pronoun',\n",
        "    'PRP$': 'Possessive pronoun',\n",
        "    'RB': 'Adverb',\n",
        "    'RBR': 'Adverb, comparative',\n",
        "    'RBS': 'Adverb, superlative',\n",
        "    'RP': 'Particle',\n",
        "    'SYM': 'Symbol',\n",
        "    'TO': 'to',\n",
        "    'UH': 'Interjection',\n",
        "    'VB': 'Verb, base form',\n",
        "    'VBD': 'Verb, past tense',\n",
        "    'VBG': 'Verb, gerund or present participle',\n",
        "    'VBN': 'Verb, past participle',\n",
        "    'VBP': 'Verb, non-3rd person singular present',\n",
        "    'VBZ': 'Verb, 3rd person singular present',\n",
        "    'WDT': 'Wh-determiner',\n",
        "    'WP': 'Wh-pronoun',\n",
        "    'WP$': 'Possessive wh-pronoun',\n",
        "    'WRB': 'Wh-adverb'\n",
        "}\n"
      ],
      "metadata": {
        "id": "2WrbNXVRE72i"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "df_pos['POS Full Form'] = df_pos['POS'].map(pos_tag_full_form)\n",
        "\n",
        "# Display the DataFrame with the new column\n",
        "print(df_pos.head())"
      ],
      "metadata": {
        "id": "OmsOqYHXE-UB"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Count total occurrences of words for each POS tag\n",
        "pos_total_counts = df_pos.groupby('POS').size()\n",
        "\n",
        "# Display the total occurrences of words for each POS tag\n",
        "print(pos_total_counts)"
      ],
      "metadata": {
        "id": "1RBf-OtMF-tA"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "# Your Turn\n",
        "Conduct POS analysis for `sms_spam.csv` and `oct_delta.csv` data."
      ],
      "metadata": {
        "id": "pjx8eOqNL4Ss"
      }
    }
  ]
}