{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "provenance": [],
      "authorship_tag": "ABX9TyMWND3ktWyexDRbwYJxUnkF",
      "include_colab_link": true
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/RDGopal/IB9CW0-Text-Analytics/blob/main/POS.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "# Part-of-Speech (POS)\n",
        "Part-of-Speech (POS) tagging is a fundamental technique in Natural Language Processing (NLP) that involves assigning a word in a text corpus to a particular part of speech, based on both its definition and its context in a sentence. POS tagging is useful for syntactic parsing and text analysis.\n",
        "\n",
        "##Concepts\n",
        "**Definition and Usage**: Each word in a sentence is tagged with a part of speech that explains its usage and function in that sentence. Common parts of speech include noun, verb, adjective, adverb, pronoun, preposition, conjunction, and interjection.\n",
        "\n",
        "**Tag Sets**: There are different sets of tags, but one of the most commonly used is the Penn Treebank tagset. Examples include:\n",
        "\n",
        "NN: Noun, singular\n",
        "NNS: Noun, plural\n",
        "VB: Verb, base form\n",
        "VBD: Verb, past tense\n",
        "JJ: Adjective\n",
        "RB: Adverb\n",
        "Techniques for POS Tagging:\n",
        "\n",
        "**Rule-Based POS Tagging**: Uses hand-written rules to distinguish the POS based on the word itself and its context within a sentence.\n",
        "\n",
        "**Stochastic POS Tagging**: Uses model-based systems like Hidden Markov Models (HMMs), Maximum Entropy Markov Models, or Conditional Random Fields (CRFs), where the model learns from a tagged corpus to apply tags to new sentences.\n",
        "\n",
        "**Deep Learning Methods**: Utilizes neural networks, especially recurrent neural networks (RNNs) and transformers, to perform POS tagging, often achieving superior accuracy.\n",
        "\n",
        "## Example with Explanation\n",
        "Consider the sentence: \"Apple is looking at buying U.K. startup for $1 billion.\"\n",
        "\n",
        "The POS tagging for this might look like:\n",
        "\n",
        "Apple (NNP - Proper noun, singular)\n",
        "\n",
        "is (VBZ - Verb, 3rd person singular present)\n",
        "\n",
        "looking (VBG - Verb, gerund or present participle)\n",
        "\n",
        "at (IN - Preposition or subordinating conjunction)\n",
        "\n",
        "buying (VBG - Verb, gerund or present participle)\n",
        "\n",
        "U.K. (NNP - Proper noun, singular)\n",
        "\n",
        "startup (NN - Noun, singular)\n",
        "\n",
        "for (IN - Preposition or subordinating conjunction)\n",
        "\n",
        "$1 billion (CD - Cardinal number)\n",
        "\n",
        "Each word is assigned a tag that explains how it functions grammatically within the sentence."
      ],
      "metadata": {
        "id": "5JmhxBV3PruY"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "# Algorithm\n",
        "The `averaged_perceptron_tagger` in NLTK is a type of part-of-speech tagger based on the Averaged Perceptron algorithm, a supervised learning method. It's designed to assign part-of-speech tags to each word in a sentence, based on the contextual information of the words. Averaged Perceptron algorithm works as follows:\n",
        "\n",
        "##Overview of the Perceptron\n",
        "A perceptron is a simple linear binary classifier that makes predictions based on a weighted sum of the input features. For POS tagging, the features are typically aspects of words (like the word itself, the previous word, the next word, prefixes, suffixes, etc.). The basic steps are:\n",
        "\n",
        "**Extract Features**: For each word, relevant features are extracted. These might include:\n",
        "\n",
        "* The word itself.\n",
        "* The previous word and its tag.\n",
        "* The next word.\n",
        "* Suffixes and prefixes of the word.\n",
        "* Whether the word is capitalized.\n",
        "\n",
        "\n",
        "**Weight Calculation**: Each feature has an associated weight, which determines its importance. In training, the perceptron algorithm adjusts these weights.\n",
        "\n",
        "**Decision Making**: To predict the POS tag of a word, the perceptron computes a weighted sum of the features. The tag with the highest sum (score) is chosen as the output.\n",
        "\n",
        "## Averaged Perceptron\n",
        "The Averaged Perceptron is an enhancement over the basic perceptron, designed to reduce the sensitivity to the order of training data and to provide better generalization:\n",
        "\n",
        "**Weight Update**: During training, each time the perceptron makes a mistake (the predicted tag differs from the actual tag), it updates the weights of the features. Weights that would have led to the correct tag are increased, while those that led to the wrong tag are decreased.\n",
        "\n",
        "**Averaging**: The core idea of the Averaged Perceptron is that it keeps a running average of all the weight values throughout training. Instead of using just the final values of the weights (as in a standard perceptron), it calculates the average of each weight over all updates. This averaging helps smooth out anomalies from any particular training instance and leads to better performance on unseen data.\n",
        "\n",
        "## Training and Prediction\n",
        "The training process involves multiple passes (epochs) over the training dataset, adjusting the weights based on errors. These steps are iterated until a stopping criterion is met, typically a maximum number of epochs or a minimum error threshold.\n",
        "\n",
        "When predicting POS tags:\n",
        "\n",
        "**Feature Extraction**: Extract the same types of features used during training.\n",
        "\n",
        "**Score Calculation**: Compute scores for each tag based on the current weights and the extracted features.\n",
        "\n",
        "**Tag Selection**: Select the tag with the highest score."
      ],
      "metadata": {
        "id": "pzokcJrPRMI8"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "# Training\n",
        "The averaged_perceptron_tagger in NLTK is typically trained on the Wall Street Journal (WSJ) portion of the Penn Treebank dataset. This dataset is one of the most widely used corpora for training linguistic models in English, particularly for tasks involving part-of-speech tagging and syntactic parsing.\n",
        "\n",
        "## Penn Treebank Dataset\n",
        "**Overview**: The Penn Treebank, developed in the collaborations between the University of Pennsylvania and IBM in the early 1990s, is a corpus that annotates syntactic or predicate-argument structure of English sentences. This corpus includes a diverse range of texts from the Wall Street Journal (WSJ), as well as other materials.\n",
        "\n",
        "**Content**: For POS tagging, specifically, the portion from the Wall Street Journal is commonly used because it covers a wide variety of topics in financial and economic news, making the vocabulary and grammatical structures varied and rich.\n",
        "\n",
        "**Annotations**: Each word in the dataset is annotated with a POS tag based on the Penn Treebank tagset, which includes detailed tags not only for traditional parts of speech like nouns and verbs but also for more specific categories like cardinal numbers and different verb forms.\n",
        "\n",
        "## Training Process\n",
        "The training of the averaged_perceptron_tagger involves the following steps using this dataset:\n",
        "\n",
        "**Feature Extraction**: The model extracts features from the training sentences, which might include the word itself, the context of the word (e.g., previous word, next word), prefixes, suffixes, and whether the word is capitalized.\n",
        "\n",
        "**Learning Weights**: As it processes each word in the training data, the model updates the weights associated with each feature based on whether its current predictions are correct. If a prediction is wrong, the model adjusts the weights to be more likely to make the correct prediction next time.\n",
        "\n",
        "**Averaging Weights**: Throughout training, the model maintains an average for each weight to ensure that the final model isnâ€™t too biased towards the latter stages of the training process or specific quirks of the training data order.\n",
        "\n",
        "## Benefits of Using Penn Treebank\n",
        "Using the Penn Treebank, especially the WSJ corpus, provides several advantages:\n",
        "\n",
        "**High Quality of Annotation**: The data is manually annotated by linguistic experts, ensuring high-quality and reliable annotations.\n",
        "\n",
        "**Diversity of Examples**: The corpus contains a wide range of sentence structures and vocabulary, which helps in building a robust model capable of handling various real-world texts.\n",
        "\n",
        "**Standardization**: Because many linguistic models are trained on this corpus, it provides a common benchmark for comparing the performance of different algorithms and approaches."
      ],
      "metadata": {
        "id": "iklhlPjJSd8T"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "pos_tag_full_form = {\n",
        "    'CC': 'Coordinating conjunction',\n",
        "    'CD': 'Cardinal number',\n",
        "    'DT': 'Determiner',\n",
        "    'EX': 'Existential there',\n",
        "    'FW': 'Foreign word',\n",
        "    'IN': 'Preposition or subordinating conjunction',\n",
        "    'JJ': 'Adjective',\n",
        "    'JJR': 'Adjective, comparative',\n",
        "    'JJS': 'Adjective, superlative',\n",
        "    'LS': 'List item marker',\n",
        "    'MD': 'Modal',\n",
        "    'NN': 'Noun, singular or mass',\n",
        "    'NNS': 'Noun, plural',\n",
        "    'NNP': 'Proper noun, singular',\n",
        "    'NNPS': 'Proper noun, plural',\n",
        "    'PDT': 'Predeterminer',\n",
        "    'POS': 'Possessive ending',\n",
        "    'PRP': 'Personal pronoun',\n",
        "    'PRP$': 'Possessive pronoun',\n",
        "    'RB': 'Adverb',\n",
        "    'RBR': 'Adverb, comparative',\n",
        "    'RBS': 'Adverb, superlative',\n",
        "    'RP': 'Particle',\n",
        "    'SYM': 'Symbol',\n",
        "    'TO': 'to',\n",
        "    'UH': 'Interjection',\n",
        "    'VB': 'Verb, base form',\n",
        "    'VBD': 'Verb, past tense',\n",
        "    'VBG': 'Verb, gerund or present participle',\n",
        "    'VBN': 'Verb, past participle',\n",
        "    'VBP': 'Verb, non-3rd person singular present',\n",
        "    'VBZ': 'Verb, 3rd person singular present',\n",
        "    'WDT': 'Wh-determiner',\n",
        "    'WP': 'Wh-pronoun',\n",
        "    'WP$': 'Possessive wh-pronoun',\n",
        "    'WRB': 'Wh-adverb'\n",
        "}"
      ],
      "metadata": {
        "id": "2WrbNXVRE72i"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "import nltk\n",
        "import pandas as pd\n",
        "\n",
        "nltk.download('averaged_perceptron_tagger')\n",
        "nltk.download('punkt')"
      ],
      "metadata": {
        "id": "nofUw63sWaie"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Example text\n",
        "sentence = \"The quick brown fox jumps over the lazy dog\"\n",
        "\n",
        "# Tokenizing the sentence\n",
        "tokens = nltk.word_tokenize(sentence)\n",
        "\n",
        "# POS Tagging\n",
        "tags = nltk.pos_tag(tokens)\n",
        "print(tags)\n"
      ],
      "metadata": {
        "id": "lnsKTz5iWMX2"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Create the DataFrame\n",
        "df_pos = pd.DataFrame(tags)\n",
        "df_pos.columns = ['Word','POS']"
      ],
      "metadata": {
        "id": "ZwsZQRs6XNvH"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Add POS explainers\n",
        "df_pos['POS Full Form'] = df_pos['POS'].map(pos_tag_full_form)\n",
        "df_pos"
      ],
      "metadata": {
        "id": "0xK052AaXh3-"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "# Context-Dependent POS Tagging:\n",
        "**Word**: \"set\"\n",
        "\n",
        "**As a Noun**: \"I have a chess set at home.\" (NN - Noun, singular)\n",
        "\n",
        "**As a Verb**: \"Please set the table for dinner.\" (VB - Verb, base form)\n",
        "\n",
        "**Word**: \"can\"\n",
        "\n",
        "**As a Modal Verb**: \"You can see the stars tonight.\" (MD - Modal)\n",
        "\n",
        "**As a Noun**: \"Throw it in the trash can.\" (NN - Noun, singular)\n",
        "\n",
        "**Word**: \"right\"\n",
        "\n",
        "**As an Adverb**: \"Turn right at the corner.\" (RB - Adverb)\n",
        "\n",
        "**As an Adjective**: \"He is the right person for the job.\" (JJ - Adjective)\n",
        "\n",
        "**As a Noun**: \"They fought for their rights.\" (NNS - Noun, plural)\n",
        "\n",
        "## How POS Tagging Handles Context\n",
        "POS tagging models, especially those that are context-aware (like HMMs, Conditional Random Fields, or neural network-based models), determine the part of speech for each word by considering:\n",
        "\n",
        "* The word itself: Some words are more likely to be associated with certain parts of speech.\n",
        "* Contextual clues: The words surrounding a particular word give strong hints about its likely part of speech.\n",
        "* Word endings: For example, words ending in \"-ing\" are often verbs.\n",
        "* Grammatical patterns: Certain common patterns or sequences of tags are more likely than others, which guides the tagging process."
      ],
      "metadata": {
        "id": "89DDQUelZShm"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "from nltk.tokenize import word_tokenize\n",
        "sentences = [\n",
        "    \"It is a good book\",\n",
        "    \"Can you book the ticket?\",\n",
        "    \"That is a trash can\",\n",
        "    \"I love fresh air\",\n",
        "    \"can you air out the room?\"\n",
        "]\n",
        "\n",
        "# Tokenize and POS tag each sentence\n",
        "for sentence in sentences:\n",
        "    tokens = word_tokenize(sentence)\n",
        "    tags = nltk.pos_tag(tokens)\n",
        "    print(tags)"
      ],
      "metadata": {
        "id": "30RTrXecZ1Jh"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "# SpaCy\n",
        "SpaCy and NLTK (Natural Language Toolkit) are two of the most popular libraries in Python for natural language processing (NLP), but they are designed with different goals and use cases in mind. Here's a detailed comparison highlighting their key differences:\n",
        "\n",
        "## Design Philosophy and Use Case\n",
        "### SpaCy:\n",
        "\n",
        "Performance-Oriented: SpaCy is built with performance in mind, both in terms of processing speed and accuracy. It is designed for practical, real-world system application and is often preferred in industry settings where performance and scalability are crucial.\n",
        "\n",
        "Modern and Minimalist: Provides a clear and concise API that focuses on common use cases, making it easier to implement complex NLP pipelines with fewer lines of code.\n",
        "\n",
        "Production-Ready: Includes pre-trained models that are optimized for more efficient performance, making it more suitable for deployment in production environments.\n",
        "\n",
        "###NLTK:\n",
        "\n",
        "Educational Tool: Originally designed as an educational and research tool, NLTK is excellent for teaching and studying NLP concepts, including traditional NLP tasks.\n",
        "\n",
        "Comprehensive Toolkit: Offers a vast array of algorithms for almost every NLP task and is ideal for experimenting with different methods, especially in an academic or research setting.\n",
        "\n",
        "Modular and Extensive: Contains a wide range of modules and datasets, providing tools for almost every computational task related to human language.\n",
        "\n",
        "##Features and Capabilities\n",
        "###SpaCy:\n",
        "\n",
        "Built-In Word Vectors: SpaCy supports word vectors natively and includes functions to leverage these for various tasks.\n",
        "\n",
        "Dependency Parsing: Comes with a highly efficient and accurate syntactic dependency parser.\n",
        "\n",
        "Entity Recognition: Has strong support for named entity recognition with pre-trained models.\n",
        "\n",
        "Pipeline Customization: Allows for easy customization and extension of processing pipelines to include custom components or models.\n",
        "\n",
        "###NLTK:\n",
        "\n",
        "Text Processing Libraries: Includes a broad spectrum of libraries for tokenization, stemming, tagging, parsing, and semantic reasoning, which is more comprehensive than SpaCy's.\n",
        "\n",
        "Corpora and Resources: Ships with a large suite of text corpora and lexical resources, facilitating a wide variety of NLP tasks.\n",
        "\n",
        "Prototyping: Provides more flexibility for NLP prototyping, especially for less common or more experimental techniques.\n",
        "\n",
        "## Performance\n",
        "SpaCy is generally faster and more memory-efficient than NLTK, due to its Cython-based implementation, which makes it suitable for high-volume and real-time applications.\n",
        "\n",
        "NLTK can be slower and less efficient, making it less ideal for production but excellent for teaching and prototyping, where performance is often a secondary concern.\n",
        "\n",
        "##Practical Implementation\n",
        "SpaCy is often chosen for commercial software and production systems due to its performance and ease of integration into applications.\n",
        "\n",
        "NLTK is typically used in academic settings or in scenarios where one needs to try a variety of algorithms or conduct comprehensive linguistic research."
      ],
      "metadata": {
        "id": "H7mtyLYz7NnP"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "In SpaCy, when you process a text through its NLP pipeline, each word in the text is converted into a Token object. Each Token object has various attributes that provide detailed information about the word. Here's what each of the attributes represents in the context of a SpaCy Token:\n",
        "\n",
        "`token.text`:\n",
        "\n",
        "This attribute returns the exact text of the token or word as it appeared in the input text.\n",
        "\n",
        "`token.lemma_`:\n",
        "\n",
        "The lemma of the token is its base form or dictionary form. For example, the lemma of \"was\" is \"be\", and the lemma of \"mice\" is \"mouse\". This is useful for normalizing the text and reducing the number of forms you need to deal with in NLP applications.\n",
        "\n",
        "`token.pos_`:\n",
        "\n",
        "Stands for \"Part Of Speech\". This attribute returns the simple part-of-speech tag of the token (like VERB, NOUN, ADJECTIVE, etc.). It's a coarse-grained POS tag.\n",
        "\n",
        "`token.tag_`:\n",
        "\n",
        "This provides the detailed part-of-speech tag. It's more specific than pos_ and uses a tag set specific to the language (such as the Penn Treebank tag set for English). For example, NN for singular noun, NNS for plural noun, etc.\n",
        "\n",
        "`token.dep_`:\n",
        "\n",
        "This stands for \"syntactic dependency\". It indicates the relation between this token and the token it is attached to (its head). Examples include nsubj (nominal subject), dobj (direct object), or amod (adjective modifier).\n",
        "token.shape_:\n",
        "\n",
        "This attribute returns a transformation of the token text that shows its general shape by replacing lowercase letters with 'x', uppercase letters with 'X', and digits with 'd'. For example, \"Apple123\" becomes \"Xxxxxddd\".\n",
        "\n",
        "`token.is_alpha`:\n",
        "\n",
        "A boolean attribute that returns True if the token consists of only alphabetic characters (no digits or punctuation). For example, \"Apple\" would return True, but \"Apple123\" would return False.\n",
        "\n",
        "`token.is_stop`:\n",
        "\n",
        "Another boolean attribute that indicates whether the token is a stop word (a common word that may be filtered out in some types of text processing, such as \"and\", \"the\", or \"is\" in English). SpaCy has a built-in list of stop words for each language."
      ],
      "metadata": {
        "id": "no7sMj_h9q9v"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "import spacy\n",
        "\n",
        "# Load English tokenizer, tagger, parser, NER and word vectors\n",
        "nlp = spacy.load('en_core_web_sm')\n",
        "\n",
        "text = \"Apple is looking at buying U.K. startup for $1 billion\"\n",
        "doc = nlp(text)\n",
        "\n",
        "for token in doc:\n",
        "    print(token.text, token.lemma_, token.pos_, token.tag_, token.dep_,\n",
        "            token.shape_, token.is_alpha, token.is_stop)\n"
      ],
      "metadata": {
        "id": "xuCB7HBy76IG"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "import spacy\n",
        "\n",
        "# Load the English NLP model\n",
        "nlp = spacy.load('en_core_web_sm')\n",
        "\n",
        "sentences = [\n",
        "    \"It is a good book\",\n",
        "    \"Can you book the ticket?\",\n",
        "    \"That is a trash can\",\n",
        "    \"I love fresh air\",\n",
        "    \"can you air out the room?\"\n",
        "]\n",
        "\n",
        "# Process the sentences and print POS tags\n",
        "for sentence in sentences:\n",
        "    doc = nlp(sentence)\n",
        "    print([(token.text, token.pos_) for token in doc])\n"
      ],
      "metadata": {
        "id": "s45PGRTabt8l"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "# BERT based pos\n"
      ],
      "metadata": {
        "id": "ImmKnmgV-lfW"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "BERT's architecture allows it to understand the context of each word in a sentence more effectively than traditional models, which is particularly beneficial for tasks like POS tagging.\n",
        "\n",
        "##How BERT is Used for POS Tagging\n",
        "BERT can be fine-tuned for specific tasks including POS tagging. The process typically involves:\n",
        "\n",
        "**Pre-training**: BERT models are pre-trained on a large corpus of text with tasks like masked language modeling (MLM) and next sentence prediction (NSP). This pre-training helps the model understand language deeply.\n",
        "\n",
        "**Fine-Tuning**: For POS tagging, BERT is fine-tuned on a labeled dataset where each word in a sentence is tagged with its correct part of speech. During fine-tuning, the output layer of BERT is adapted to predict POS tags instead of its original pre-training tasks.\n",
        "\n",
        "##Hugging Face Transformers\n",
        "The Hugging Face transformers library provides an accessible way to use pre-trained BERT models and fine-tune them for various tasks including POS tagging."
      ],
      "metadata": {
        "id": "xhBW5eSHDwnF"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "pip install transformers"
      ],
      "metadata": {
        "id": "qOPNoH4Y-qKe"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "from transformers import AutoTokenizer, AutoModelForTokenClassification, TokenClassificationPipeline\n",
        "\n",
        "model_name = \"QCRI/bert-base-multilingual-cased-pos-english\"\n",
        "tokenizer = AutoTokenizer.from_pretrained(model_name)\n",
        "model = AutoModelForTokenClassification.from_pretrained(model_name)\n",
        "pipeline = TokenClassificationPipeline(model=model, tokenizer=tokenizer)"
      ],
      "metadata": {
        "id": "_AQHRSEe-wSu"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "outputs = pipeline(\"It is a good book\")\n",
        "# print(outputs)\n",
        "\n",
        "# Create a new list to store just the word and entity\n",
        "filtered_outputs = [{'word': item['word'], 'entity': item['entity']} for item in outputs]\n",
        "\n",
        "# Display the filtered output\n",
        "print(filtered_outputs)"
      ],
      "metadata": {
        "id": "WdFgDof1ATFF"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "sentences = [\n",
        "    \"It is a good book\",\n",
        "    \"Can you book the ticket?\",\n",
        "    \"That is a trash can\",\n",
        "    \"I love fresh air\",\n",
        "    \"can you air out the room?\"\n",
        "]\n",
        "\n",
        "# Process the sentences and print POS tags\n",
        "for sentence in sentences:\n",
        "    outputs = pipeline(sentence)\n",
        "    filtered_outputs = [{item['word'],item['entity']} for item in outputs]\n",
        "    print(filtered_outputs)"
      ],
      "metadata": {
        "id": "PGuVFfdICAA-"
      },
      "execution_count": null,
      "outputs": []
    }
  ]
}