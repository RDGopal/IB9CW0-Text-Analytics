{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "provenance": [],
      "authorship_tag": "ABX9TyNYk5iwNF4XKt2IUADs4W56",
      "include_colab_link": true
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/RDGopal/IB9CW0-Text-Analytics/blob/main/Lecture_7.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "# Prediction and Semantic Search with Embeddings"
      ],
      "metadata": {
        "id": "hOEzTleIbIch"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "## Prediction\n",
        "We will continue with the `sms_spam.csv` dataset to analyze and predict whether an SMS is spam or not. In this instance, we will use embeddings of the text to evaluate its predictive performance."
      ],
      "metadata": {
        "id": "OlfZa_WVbPOZ"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "import pandas as pd\n",
        "import nltk"
      ],
      "metadata": {
        "id": "dqqAdmflcJZR"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "df = pd.read_csv('https://raw.githubusercontent.com/RDGopal/IB9CW0-Text-Analytics/main/Data/sms_spam.csv')\n",
        "df"
      ],
      "metadata": {
        "id": "Gt9b4hT3cVax"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "##Sentence Embeddings\n",
        "Sentence embeddings are high-dimensional vector representations of sentences, capturing the semantic properties of the text. Unlike word embeddings that represent individual words, sentence embeddings represent the entire input sentence.\n",
        "\n",
        "###Sentence Embeddings vs. Word Embeddings\n",
        "\n",
        "* Granularity: Word embeddings represent individual words, whereas sentence embeddings encapsulate the meaning of full sentences or even larger text chunks.\n",
        "* Context Sensitivity: Word embeddings have a fixed representation for each word, regardless of its contextual use. Sentence embeddings, on the other hand, consider the context of the entire sentence, which can change the representation based on how words are used together.\n",
        "* Use Cases: Word embeddings are useful for tasks like word similarity and word analogy, while sentence embeddings are better suited for tasks that require understanding of larger text units, such as document classification, sentiment analysis, and question answering."
      ],
      "metadata": {
        "id": "-jdlEZpQcqUP"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "##Doc2Vec: Computation of Embeddings\n",
        "* Mechanism: Developed as an extension of the Word2Vec model, Doc2Vec (also known as Paragraph Vector) embeds words in a vector space and adds a unique vector (document ID) that represents the document (or sentence) itself. It learns to predict words in a document while also maintaining a unique document vector.\n",
        "* Training: Doc2Vec can be trained using two architectures:\n",
        "** Distributed Memory (DM): Similar to Word2Vec’s CBOW model, but adds a paragraph token.\n",
        "** Distributed Bag of Words (DBOW): Similar to Word2Vec’s Skip-gram, but predicts words randomly sampled from the paragraph, ignoring context words.\n",
        "\n",
        "By default, Gensim’s Doc2Vec uses the Distributed Memory (DM) model. This is one of the two primary algorithms for training Doc2Vec, and it works by preserving the order of words in the document while attempting to predict a word in the context of the preceding words and a special token that represents the document (or sentence). This model is analogous to the Continuous Bag of Words (CBOW) model used in Word2Vec, but with the addition of the paragraph (document) vector.\n",
        "\n",
        "**Key Characteristics of DM:**\n",
        "\n",
        "It attempts to predict a word based on the context words and a unique document identifier. It generally produces more coherent embeddings for larger documents where the order of words contributes more meaningfully to the semantic content.\n",
        "\n",
        "If you wanted to use the other training method, the Distributed Bag of Words (DBOW), you would need to specify this when initializing the Doc2Vec model with the parameter `dm=0`:\n",
        "\n",
        "\n",
        "`model = Doc2Vec(vector_size=40, min_count=2, epochs=30, dm=0)`\n",
        "\n",
        "**Comparison of DM and DBOW:**\n",
        "\n",
        "* DM (Distributed Memory):\n",
        "Better for understanding semantic similarity.\n",
        "Uses the context of the current word to predict the word.\n",
        "Typically results in higher quality embeddings where document order matters.\n",
        "* DBOW (Distributed Bag of Words):\n",
        "Does not need the word order, thus faster to train.\n",
        "It predicts words randomly from the paragraph in the current context.\n",
        "Can be less memory intensive as it does not need to store word vectors during training.\n",
        "\n",
        "\n",
        "The choice between DM and DBOW often depends on the specific requirements of the application, the nature of the data, and computational resources. DM is usually preferred when the quality of the embeddings is paramount, while DBOW can be favored for its speed and lower resource consumption.\n",
        "\n",
        "We will create 40 dimensional doc2vec embeddings of the text and use these for predicting the outcome."
      ],
      "metadata": {
        "id": "o4A8hJxLya3d"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "from gensim.models import Doc2Vec\n",
        "from gensim.models.doc2vec import TaggedDocument\n",
        "from nltk.tokenize import word_tokenize\n",
        "import nltk\n",
        "nltk.download('punkt')\n",
        "\n",
        "# Preprocess and tag each message in the dataset\n",
        "tagged_data = [TaggedDocument(words=word_tokenize(_d.lower()), tags=[str(i)]) for i, _d in enumerate(df['text'])]\n"
      ],
      "metadata": {
        "id": "in0ZiiOkgSgH"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "from gensim.models import Doc2Vec\n",
        "from gensim.models.doc2vec import TaggedDocument\n",
        "from nltk.tokenize import word_tokenize\n",
        "import nltk\n",
        "nltk.download('punkt')\n",
        "\n",
        "# Preprocess and tag each message in the dataset\n",
        "tagged_data = [TaggedDocument(words=word_tokenize(_d.lower()), tags=[str(i)]) for i, _d in enumerate(df['text'])]\n",
        "\n",
        "# Define and train the Doc2Vec model\n",
        "model = Doc2Vec(vector_size=40, min_count=2, epochs=30)\n",
        "model.build_vocab(tagged_data)\n",
        "model.train(tagged_data, total_examples=model.corpus_count, epochs=model.epochs)\n",
        "\n",
        "# Create embeddings and expand each element into separate columns\n",
        "embeddings = [model.infer_vector(word_tokenize(text.lower())) for text in df['text']]\n",
        "df_embeddings = pd.DataFrame(embeddings, columns=[f'embed_{i}' for i in range(len(embeddings[0]))])\n",
        "df = pd.concat([df, df_embeddings], axis=1)"
      ],
      "metadata": {
        "id": "z7P87BaIgaGv"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "df"
      ],
      "metadata": {
        "id": "_vaVEAdDg17w"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "## Prediction with Doc2Vec Embeddings"
      ],
      "metadata": {
        "id": "gwJuM3Ip2MtN"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "from sklearn.ensemble import RandomForestClassifier\n",
        "from sklearn.model_selection import train_test_split\n",
        "from sklearn.metrics import confusion_matrix, roc_curve, auc, classification_report\n",
        "import matplotlib.pyplot as plt\n",
        "import numpy as np\n",
        "\n",
        "\n",
        "\n",
        "# Select embedding columns as features. This assumes embedding column names are like 'embed_0', 'embed_1', ..., 'embed_39'\n",
        "X = df.loc[:, df.columns.str.startswith('embed_')]\n",
        "y = df['type'].apply(lambda x: 1 if x == 'spam' else 0)\n",
        "\n",
        "# Split data into training and testing sets\n",
        "X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.2, random_state=42)\n",
        "\n",
        "# Initialize and train the Random Forest classifier\n",
        "rf_classifier = RandomForestClassifier(n_estimators=100, random_state=42)\n",
        "rf_classifier.fit(X_train, y_train)\n",
        "\n",
        "# Predict on the test set\n",
        "y_pred = rf_classifier.predict(X_test)\n",
        "y_prob = rf_classifier.predict_proba(X_test)[:, 1]  # probabilities for ROC curve\n",
        "\n",
        "# Confusion Matrix\n",
        "conf_matrix = confusion_matrix(y_test, y_pred)\n",
        "print(\"Confusion Matrix:\\n\", conf_matrix)\n",
        "\n",
        "# ROC Curve\n",
        "fpr, tpr, _ = roc_curve(y_test, y_prob)\n",
        "roc_auc = auc(fpr, tpr)\n",
        "\n",
        "plt.figure()\n",
        "plt.plot(fpr, tpr, color='darkorange', lw=2, label='ROC curve (area = %0.2f)' % roc_auc)\n",
        "plt.plot([0, 1], [0, 1], color='navy', lw=2, linestyle='--')\n",
        "plt.xlim([0.0, 1.0])\n",
        "plt.ylim([0.0, 1.05])\n",
        "plt.xlabel('False Positive Rate')\n",
        "plt.ylabel('True Positive Rate')\n",
        "plt.title('Receiver Operating Characteristic')\n",
        "plt.legend(loc=\"lower right\")\n",
        "plt.show()\n",
        "\n",
        "# Feature Importance\n",
        "importances = rf_classifier.feature_importances_\n",
        "indices = np.argsort(importances)[::-1]\n",
        "print(\"Feature ranking:\")\n",
        "\n",
        "for f in range(X_train.shape[1]):\n",
        "    print(\"%d. embedding %d (%f)\" % (f + 1, indices[f], importances[indices[f]]))\n"
      ],
      "metadata": {
        "id": "QWFlcwszjEcP"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "We notice that embeddings are slightly better predictors than tf-idf values we used before."
      ],
      "metadata": {
        "id": "K1q3FMvjjyz_"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "##Sentence Transformers: Computation of Embeddings\n",
        "\n",
        "* Mechanism: Sentence-transformers modify the pre-trained BERT or other transformer models to produce meaningful sentence embeddings. It uses siamese and triplet network structures to derive semantically meaningful sentence embeddings that can be compared using cosine similarity.\n",
        "* Training: It typically involves fine-tuning a transformer model on a dataset of sentence pairs with some similarity measure. The goal is to train embeddings such that similar sentences are close in vector space, and dissimilar sentences are far apart.\n",
        "\n",
        "###Advantages and Disadvantages\n",
        "\n",
        "**Doc2Vec:**\n",
        "\n",
        "* Advantages:\n",
        "** Good at capturing semantic meaning of longer texts.\n",
        "** Does not require labeled data, as it uses unsupervised learning.\n",
        "\n",
        "* Disadvantages:\n",
        "** Inferior in capturing nuances compared to more advanced models like BERT.\n",
        "** Requires careful hyperparameter tuning and significant training data to perform well.\n",
        "\n",
        "**Sentence-Transformers**:\n",
        "\n",
        "* Advantages:\n",
        "** Produces state-of-the-art embeddings that are highly effective for many NLP tasks.\n",
        "** Can leverage pre-trained transformer models which have been trained on vast amounts of data.\n",
        "* Disadvantages:\n",
        "** Computationally expensive, requiring powerful hardware (GPUs) for fine-tuning and inference.\n",
        "** Sometimes overfitting can occur on smaller or less diverse datasets.\n",
        "\n",
        "Models in sentence-transformers have fixed embedding sizes determined by their architecture. For example, models based on BERT typically produce embeddings of size 768, whereas smaller models like `all-MiniLM-L6-v2` produce embeddings of size 384."
      ],
      "metadata": {
        "id": "37c5lUejj-q2"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "df = pd.read_csv('https://raw.githubusercontent.com/RDGopal/IB9CW0-Text-Analytics/main/Data/sms_spam.csv')"
      ],
      "metadata": {
        "id": "dqbrLI-_kE42"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "%pip install -U sentence-transformers"
      ],
      "metadata": {
        "id": "gcB9acBNLDOH"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# !pip install -U sentence-transformers\n",
        "\n",
        "from sentence_transformers import SentenceTransformer\n",
        "\n",
        "# Load a pre-trained model\n",
        "model = SentenceTransformer('all-MiniLM-L6-v2')\n",
        "\n",
        "# Generate embeddings and expand into separate columns\n",
        "hf_embeddings = [model.encode(text) for text in df['text']]\n",
        "df_hf_embeddings = pd.DataFrame(hf_embeddings, columns=[f'hf_embed_{i}' for i in range(len(hf_embeddings[0]))])\n",
        "df = pd.concat([df, df_hf_embeddings], axis=1)\n",
        "\n",
        "# Check the dataframe\n",
        "print(df.head())\n"
      ],
      "metadata": {
        "id": "DXYQ9csekRqf"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "df"
      ],
      "metadata": {
        "id": "Nw16bseAmMo_"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "### Reduce dimensionality\n",
        "Sometimes you want to reduce the dimensionality of the embedding vector. In our case, let's say that we want to reduce from 384 dimensions to 40 dimensions. The basic approach is to apply dimensionality reduction techniques like PCA after generating embeddings."
      ],
      "metadata": {
        "id": "f7cbX0ugnfVm"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "df = pd.read_csv('https://raw.githubusercontent.com/RDGopal/IB9CW0-Text-Analytics/main/Data/sms_spam.csv')"
      ],
      "metadata": {
        "id": "y1Y61wsKnjx2"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "import pandas as pd\n",
        "from sentence_transformers import SentenceTransformer\n",
        "from sklearn.decomposition import PCA\n",
        "\n",
        "# Load a pre-trained model from sentence-transformers\n",
        "model = SentenceTransformer('all-MiniLM-L6-v2')\n",
        "\n",
        "# Generate embeddings for all texts\n",
        "hf_embeddings = [model.encode(text) for text in df['text']]\n",
        "\n",
        "# Convert list of embeddings into a DataFrame\n",
        "embeddings_df = pd.DataFrame(hf_embeddings)\n",
        "\n",
        "# Initialize PCA to reduce to 40 dimensions\n",
        "pca = PCA(n_components=40)\n",
        "pca_result = pca.fit_transform(embeddings_df.values)\n",
        "\n",
        "# Convert the PCA result into a DataFrame and set appropriate column names\n",
        "df_pca_embeddings = pd.DataFrame(pca_result, columns=[f'pca_embed_{i}' for i in range(40)])\n",
        "\n",
        "# Drop any existing PCA embedding columns first to avoid duplication\n",
        "df = df.drop(columns=[col for col in df.columns if col.startswith('pca_embed_')], errors='ignore')\n",
        "\n",
        "# Concatenate the original DataFrame with the PCA embeddings DataFrame\n",
        "df = pd.concat([df, df_pca_embeddings], axis=1)\n",
        "\n",
        "# Check the new DataFrame structure\n",
        "print(df.head())\n"
      ],
      "metadata": {
        "id": "qjXfQ78mo-c2"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "## Prediction with Sentence-Transformers"
      ],
      "metadata": {
        "id": "R2ZBOYpc16bM"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "from sklearn.ensemble import RandomForestClassifier\n",
        "from sklearn.model_selection import train_test_split\n",
        "from sklearn.metrics import confusion_matrix, roc_curve, auc, classification_report\n",
        "import matplotlib.pyplot as plt\n",
        "import numpy as np\n",
        "\n",
        "\n",
        "# Assuming 'df' is the DataFrame and the target variable 'type' is encoded as 0 for 'ham' and 1 for 'spam'\n",
        "# Select embedding columns as features. This assumes embedding column names are like 'embed_0', 'embed_1', ..., 'embed_39'\n",
        "X = df.loc[:, df.columns.str.startswith('pca_embed_')]\n",
        "y = df['type'].apply(lambda x: 1 if x == 'spam' else 0)\n",
        "\n",
        "# Split data into training and testing sets\n",
        "X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.2, random_state=42)\n",
        "\n",
        "# Initialize and train the Random Forest classifier\n",
        "rf_classifier = RandomForestClassifier(n_estimators=100, random_state=42)\n",
        "rf_classifier.fit(X_train, y_train)\n",
        "\n",
        "# Predict on the test set\n",
        "y_pred = rf_classifier.predict(X_test)\n",
        "y_prob = rf_classifier.predict_proba(X_test)[:, 1]  # probabilities for ROC curve\n",
        "\n",
        "# Confusion Matrix\n",
        "conf_matrix = confusion_matrix(y_test, y_pred)\n",
        "print(\"Confusion Matrix:\\n\", conf_matrix)\n",
        "\n",
        "# ROC Curve\n",
        "fpr, tpr, _ = roc_curve(y_test, y_prob)\n",
        "roc_auc = auc(fpr, tpr)\n",
        "\n",
        "plt.figure()\n",
        "plt.plot(fpr, tpr, color='darkorange', lw=2, label='ROC curve (area = %0.2f)' % roc_auc)\n",
        "plt.plot([0, 1], [0, 1], color='navy', lw=2, linestyle='--')\n",
        "plt.xlim([0.0, 1.0])\n",
        "plt.ylim([0.0, 1.05])\n",
        "plt.xlabel('False Positive Rate')\n",
        "plt.ylabel('True Positive Rate')\n",
        "plt.title('Receiver Operating Characteristic')\n",
        "plt.legend(loc=\"lower right\")\n",
        "plt.show()\n",
        "\n",
        "# Feature Importance\n",
        "importances = rf_classifier.feature_importances_\n",
        "indices = np.argsort(importances)[::-1]\n",
        "print(\"Feature ranking:\")\n",
        "\n",
        "for f in range(X_train.shape[1]):\n",
        "    print(\"%d. embedding %d (%f)\" % (f + 1, indices[f], importances[indices[f]]))"
      ],
      "metadata": {
        "id": "_ODT0f5dv3B8"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "# Your Turn\n",
        "Read the `imdb.csv` file and predict the sentiment based on the embeddings of the review of the movie."
      ],
      "metadata": {
        "id": "64FlHo6_YpSP"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "##Semantic Search\n",
        "Semantic search is an approach that aims to understand the meaning behind a search query, instead of just matching keywords, in order to return more relevant results. We can use  embeddings, which are vector representations of underlying text that capture their meaning, to represent the meaning of a search query. In this exercise, we will perform a semantic search using embeddings.\n"
      ],
      "metadata": {
        "id": "piUCxmKMEGeC"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "### Read the PDF text\n",
        "* Read the PDF document `docAI.pdf`.\n",
        "* Split the text into sentences and store in a dataframe.\n",
        "* Clean and preprocess the text.\n"
      ],
      "metadata": {
        "id": "zNjdvEnLSXa4"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "%pip install PyPDF2"
      ],
      "metadata": {
        "id": "vJFdd236FGhK"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "### Choice 1: Read the pdf locally"
      ],
      "metadata": {
        "id": "NtPp7mFfFleG"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "import pandas as pd\n",
        "import numpy as np\n",
        "from PyPDF2 import PdfReader\n",
        "import re\n",
        "\n",
        "# Read the PDF file\n",
        "reader = PdfReader(\"docAI.pdf\")\n",
        "\n",
        "text = \"\"\n",
        "for page in reader.pages:\n",
        "    text += page.extract_text() + \" \"\n",
        "\n",
        "# Split the text into sentences\n",
        "sentences = re.split(r'\\.\\s+', text)\n",
        "\n",
        "# Clean and preprocess the text\n",
        "df = pd.DataFrame({'text': sentences})\n",
        "df['clean_text'] = df['text'].str.lower()\n",
        "df['clean_text'] = df['clean_text'].str.replace('[^a-z\\s]', '', regex=True)\n",
        "df['clean_text'] = df['clean_text'].str.replace('\\s+', ' ', regex=True)\n",
        "df['sentence_id'] = np.arange(len(df))\n"
      ],
      "metadata": {
        "id": "i33nmXW6Ed-C"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "###Choice 2: Read from GitHub"
      ],
      "metadata": {
        "id": "ucILLF0yFr2Q"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "import requests\n",
        "from PyPDF2 import PdfReader\n",
        "from io import BytesIO\n",
        "\n",
        "# URL to the raw PDF file on GitHub\n",
        "url = 'https://raw.githubusercontent.com/RDGopal/IB9CW0-Text-Analytics/main/Data/docAI.pdf'\n",
        "\n",
        "# Use requests to get the content of the PDF file\n",
        "response = requests.get(url)\n",
        "\n",
        "# Check if the request was successful\n",
        "if response.status_code == 200:\n",
        "    # Use BytesIO to convert the bytes response to a file-like object\n",
        "    pdf_file = BytesIO(response.content)\n",
        "\n",
        "    # Now you can use PdfReader to read the file\n",
        "    reader = PdfReader(pdf_file)\n",
        "\n",
        "    # Initialize a variable to hold all extracted text\n",
        "    full_text = \"\"\n",
        "\n",
        "    # Iterate over each page in the PDF file\n",
        "    for page in reader.pages:\n",
        "        text = page.extract_text()\n",
        "        if text:  # Check if text was successfully extracted\n",
        "            full_text += text + \"\\n\"  # Append the text of each page\n",
        "\n",
        "    text = full_text\n",
        "\n",
        "else:\n",
        "    print(\"Failed to download the file. Status code:\", response.status_code)"
      ],
      "metadata": {
        "id": "FMvFjUJeEIG3"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "###Split text into sentences and clean"
      ],
      "metadata": {
        "id": "ZBqQQoSVEHfn"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# Split the text into sentences\n",
        "sentences = re.split(r'\\.\\s+', text)\n",
        "\n",
        "# Clean and preprocess the text\n",
        "df = pd.DataFrame({'text': sentences})\n",
        "df['clean_text'] = df['text'].str.lower()\n",
        "df['clean_text'] = df['clean_text'].str.replace('[^a-z\\s]', '', regex=True)\n",
        "df['clean_text'] = df['clean_text'].str.replace('\\s+', ' ', regex=True)\n",
        "df['sentence_id'] = np.arange(len(df))"
      ],
      "metadata": {
        "id": "n_sMH2IRFRAm"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "df"
      ],
      "metadata": {
        "id": "57XBkehWFRZK"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "###Creating Sentence Embeddings with Doc2Vec"
      ],
      "metadata": {
        "id": "J1JmAf5CTKVw"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "from gensim.models import Doc2Vec\n",
        "from gensim.models.doc2vec import TaggedDocument\n",
        "from nltk.tokenize import word_tokenize\n",
        "\n",
        "# Tokenizing and tagging\n",
        "tagged_data = [TaggedDocument(words=word_tokenize(doc), tags=[str(i)]) for i, doc in enumerate(df['clean_text'])]\n",
        "\n",
        "# Training a Doc2Vec model\n",
        "model = Doc2Vec(vector_size=100, min_count=2, epochs=40)\n",
        "model.build_vocab(tagged_data)\n",
        "model.train(tagged_data, total_examples=model.corpus_count, epochs=model.epochs)\n",
        "\n",
        "# Saving embeddings\n",
        "df['doc2vec_embedding'] = [model.infer_vector(word_tokenize(row['clean_text'])) for index, row in df.iterrows()]\n"
      ],
      "metadata": {
        "id": "rO2NelOEFZ_h"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "###Semantic Search Functionality\n",
        "We will use cosine similarity to find the top 5 closest sentences"
      ],
      "metadata": {
        "id": "Hm1UDZ2tTSGn"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "def find_top_5_similar_sentences(df, query, model):\n",
        "    query_embedding = model.infer_vector(word_tokenize(query.lower()))\n",
        "    embeddings_matrix = np.vstack(df['doc2vec_embedding'])\n",
        "    similarities = cosine_similarity([query_embedding], embeddings_matrix)\n",
        "    top_5_indices = np.argsort(similarities[0])[::-1][:5]\n",
        "    return df.iloc[top_5_indices]['text']"
      ],
      "metadata": {
        "id": "ja3xzpI7INoZ"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "import pandas as pd\n",
        "from nltk.tokenize import word_tokenize\n",
        "from sklearn.metrics.pairwise import cosine_similarity  # Import cosine_similarity\n",
        "\n",
        "# Set display options\n",
        "pd.set_option('display.max_colwidth', None)  # Ensure no truncation\n",
        "pd.set_option('display.max_rows', None)  # Display any number of rows\n",
        "\n",
        "query = \"computing power and ChatGPT\"\n",
        "top_5_sentences = find_top_5_similar_sentences(df, query, model)\n",
        "for sentence in top_5_sentences:\n",
        "    print(sentence)\n",
        "    print()\n"
      ],
      "metadata": {
        "id": "U5PoP7exONiQ"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "### Creating Sentence Embeddings with Sentence Transformer"
      ],
      "metadata": {
        "id": "5HzC8_XvIdDJ"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "from sentence_transformers import SentenceTransformer\n",
        "\n",
        "# Load the model\n",
        "sbert_model = SentenceTransformer('all-MiniLM-L6-v2')\n",
        "\n",
        "# Create embeddings\n",
        "df['sbert_embedding'] = [sbert_model.encode(text) for text in df['clean_text']]\n"
      ],
      "metadata": {
        "id": "YK1GbItGI1Ao"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "df['clean_text']"
      ],
      "metadata": {
        "id": "2Bo8rX37NdAd"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "###Semantic Search Functionality"
      ],
      "metadata": {
        "id": "-qRMGktmT5fn"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "from sklearn.metrics.pairwise import cosine_similarity\n",
        "\n",
        "def semantic_search(query, embeddings, top_k=5):\n",
        "    query_embedding = sbert_model.encode([query])\n",
        "    cos_similarities = cosine_similarity(query_embedding, embeddings)[0]\n",
        "    top_k_indices = np.argsort(cos_similarities)[::-1][:top_k]\n",
        "    return df.iloc[top_k_indices]['clean_text']\n",
        "\n"
      ],
      "metadata": {
        "id": "Yzw4eUuMF3eh"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Set display options\n",
        "pd.set_option('display.max_colwidth', None)  # Ensure no truncation\n",
        "pd.set_option('display.max_rows', None)  # Display any number of rows\n",
        "query = \"chatGPT will change businesses\"\n",
        "top_sentences = semantic_search(query, np.array(list(df['sbert_embedding'])))\n",
        "for sentence in top_sentences:\n",
        "    print(sentence)\n",
        "    print()\n",
        "\n"
      ],
      "metadata": {
        "id": "hvadI6RMRD9g"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "Semantic search is a promising new technology that has the potential to revolutionize the way we search for information. As semantic search technology continues to develop, we can expect to see even more benefits in the future.\n",
        "\n"
      ],
      "metadata": {
        "id": "o5GLvOhKKp84"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "#YOUR TURN\n",
        "1.\tRead the document `Machine_stops.pdf` and implement semantic search over it."
      ],
      "metadata": {
        "id": "p4Dw9K8DUMPv"
      }
    }
  ]
}