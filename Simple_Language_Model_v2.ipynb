{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "provenance": [],
      "authorship_tag": "ABX9TyPSlFDpAMEwOmEzmgk40foB",
      "include_colab_link": true
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/RDGopal/IB9CW0-Text-Analytics/blob/main/Simple_Language_Model_v2.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "#Simple Language Model with Positional Embeddings\n",
        "We will expand the previous simple language model by adding positional embeddings. First we go through the preliminaries to get the model ready to train."
      ],
      "metadata": {
        "id": "3xvfc50c3q5_"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "import nltk\n",
        "import torch\n",
        "import torch.nn as nn\n",
        "import torch.optim as optim\n",
        "import numpy as np\n",
        "from gensim.models import Word2Vec\n",
        "from nltk.tokenize import word_tokenize\n",
        "nltk.download('punkt')\n",
        "import requests\n",
        "\n",
        "# URL to the raw text file on GitHub\n",
        "url = 'https://raw.githubusercontent.com/RDGopal/IB9CW0-Text-Analytics/main/Data/tinyshakespeare.txt'\n",
        "\n",
        "# Use requests to get the content of the file\n",
        "response = requests.get(url)\n",
        "\n",
        "# Ensure the request was successful\n",
        "if response.status_code == 200:\n",
        "    s_text = response.text\n",
        "    # Continue processing the text as needed\n",
        "else:\n",
        "    print(\"Failed to retrieve the file. Status code:\", response.status_code)\n",
        "\n",
        "# Print the first 500 characters\n",
        "print(s_text[:500])\n",
        "\n",
        "# Tokenize the text\n",
        "tokens = word_tokenize(s_text)\n",
        "\n",
        "# Organize the tokens into sentences, Word2Vec needs data in the format of list of lists of tokens\n",
        "sentences = [tokens[i:i+100] for i in range(0, len(tokens), 100)]\n",
        "\n",
        "# Train the CBOW model\n",
        "word2vec_model = Word2Vec(sentences, vector_size=5, window=5, min_count=1, sg=0)"
      ],
      "metadata": {
        "id": "ZWmkgwQIWnu1"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "#Tokenizer\n",
        "Once we get the embeddings, we will store the words (tokens), token ids, and embeddings in a dataframe.\n",
        "\n",
        "Note that the number of distinct words (tokens) is 14310. This is the size of our vocabulary."
      ],
      "metadata": {
        "id": "it6haWBx9wI4"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "import pandas as pd\n",
        "# Create a DataFrame to store word, token_id, and embedding\n",
        "data = {\n",
        "    'word': [],\n",
        "    'token_id': [],\n",
        "    'embedding': []\n",
        "}\n",
        "\n",
        "for idx, word in enumerate(word2vec_model.wv.index_to_key):\n",
        "    data['word'].append(word)\n",
        "    data['token_id'].append(idx)\n",
        "    data['embedding'].append(word2vec_model.wv[word].tolist())  # convert numpy array to list for easier handling in DataFrame\n",
        "\n",
        "df = pd.DataFrame(data)\n",
        "print(df)"
      ],
      "metadata": {
        "id": "ut2FlOwE90d3"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "##Positional Embeddings\n",
        "Positional embeddings are used primarily to incorporate information about the position of tokens in the input sequence into the model. The idea is to add a vector to each token's embedding that represents its position in the sequence, ensuring that the order of tokens contributes to the model's understanding.\n",
        "\n",
        "In the original Transformer architecture, positional embeddings are computed using sine and cosine functions of different frequencies:\n",
        "\n",
        "$$\n",
        "\\text{PE}(pos, 2i) = \\sin\\left(\\frac{pos}{10000^{2i/d_{\\text{model}}}}\\right) \\\\\n",
        "\\text{PE}(pos, 2i+1) = \\cos\\left(\\frac{pos}{10000^{2i/d_{\\text{model}}}}\\right)\n",
        "$$\n",
        "\n",
        "\n",
        "Where:\n",
        "\n",
        "$pos$ is the position of the token in the sequence.\n",
        "\n",
        "$i$ is the dimension index.\n",
        "\n",
        "$\n",
        "d_{model}\n",
        "$ is the dimensionality of the token embeddings.\n",
        "\n",
        "This formula helps the model to differentiate positions by providing a unique signal for each position, and the repeating patterns allow the model to generalize to sequence lengths that it has not seen before."
      ],
      "metadata": {
        "id": "bqJJlUsb4b2E"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "import numpy as np\n",
        "\n",
        "def get_positional_embeddings(sequence_length, embedding_dim):\n",
        "    positional_embeddings = np.zeros((sequence_length, embedding_dim))\n",
        "    for pos in range(sequence_length):\n",
        "        for i in range(embedding_dim):\n",
        "            if i % 2 == 0:\n",
        "                positional_embeddings[pos, i] = np.sin(pos / (10000 ** ((2 * i) / embedding_dim)))\n",
        "            else:\n",
        "                positional_embeddings[pos, i] = np.cos(pos / (10000 ** ((2 * i) / embedding_dim)))\n",
        "    return positional_embeddings"
      ],
      "metadata": {
        "id": "xIYR2FWM5Q9u"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "You can view positional embeddings as follows."
      ],
      "metadata": {
        "id": "-2G0Z3aK7Fsr"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "pos_embeddings_matrix = get_positional_embeddings(5,5)\n",
        "\n",
        "# Set the NumPy print options to suppress scientific notation\n",
        "np.set_printoptions(suppress=True, precision=3)  # Set precision as desired\n",
        "\n",
        "# Display the positional embeddings\n",
        "print(pos_embeddings_matrix)"
      ],
      "metadata": {
        "id": "99R0niYk5WQH"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "#Training Data\n",
        "Our main objective is to predict the next word (token) based on the previous 5 words (tokens). Thus, our context length is 5.\n",
        "\n",
        "We will prepare the training data such that inputs are 5 consecutive words (token) and the output to be predicted is the 6th word (token). If the input has less than 5 words (tokens), we will pad it with \\<pad>."
      ],
      "metadata": {
        "id": "sem7F-I3_qLS"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "def generate_training_data(sentences, model_wv, window_size=5):\n",
        "    embedding_dim = model_wv.vector_size\n",
        "    X, y = [], []\n",
        "    sequence_texts = []  # For storing the actual sequences of words\n",
        "    next_words = []  # For storing the actual next word\n",
        "    positional_embeddings = get_positional_embeddings(window_size, embedding_dim)\n",
        "\n",
        "    for sentence in sentences:\n",
        "        # Embed words using the Word2Vec model\n",
        "        embedded_sentence = [model_wv[word] for word in sentence if word in model_wv]\n",
        "        word_sentence = [word for word in sentence if word in model_wv]  # Keep the actual words for viewing\n",
        "\n",
        "        # Create sequences\n",
        "        for i in range(len(embedded_sentence)):\n",
        "            end_ix = i + window_size\n",
        "            if end_ix >= len(embedded_sentence):\n",
        "                break\n",
        "            seq_x, seq_y = embedded_sentence[i:end_ix], embedded_sentence[end_ix]\n",
        "            seq_text, next_word = word_sentence[i:end_ix], word_sentence[end_ix]\n",
        "\n",
        "            # Pad sequence if necessary\n",
        "            padding_length = window_size - len(seq_x)\n",
        "            seq_x += [np.zeros(embedding_dim)] * padding_length\n",
        "            seq_text += ['<pad>'] * padding_length\n",
        "\n",
        "            # Add positional embeddings\n",
        "            modified_seq_x = np.array(seq_x) + positional_embeddings[:len(seq_x)]\n",
        "\n",
        "            X.append(modified_seq_x.flatten())\n",
        "            y.append(seq_y)\n",
        "            sequence_texts.append(' '.join(seq_text))\n",
        "            next_words.append(next_word)\n",
        "\n",
        "    return np.array(X), np.array(y), sequence_texts, next_words\n",
        "\n",
        "X_train, y_train, train_sequences, train_next_words = generate_training_data(sentences, word2vec_model.wv)\n",
        "# Create DataFrame\n",
        "train_df = pd.DataFrame({\n",
        "    'Sequence': train_sequences,\n",
        "    'Next Word': train_next_words,\n",
        "    'X_train (Flattened Embeddings)': list(X_train),\n",
        "    'y_train (Embedding)': list(y_train)\n",
        "})\n"
      ],
      "metadata": {
        "id": "NGVeKomA72Tx"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "train_df.head()"
      ],
      "metadata": {
        "id": "TrMVfWPt72BE"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "Our training dataset has 241,779 data points. Each data point has 6 words (tokens), and thus the total number of words (tokens) for training is 241,779 * 6 = 1,450,674."
      ],
      "metadata": {
        "id": "oimxyIyo7DU9"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "train_df.shape\n",
        "# train_df.head()"
      ],
      "metadata": {
        "id": "0A9NtzIFA1i5"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "#Neural Network Design\n",
        "The neural network we will train has the same structure as before, with the only exception that the positional embeddings are added to the embeddings of the input words (tokens)."
      ],
      "metadata": {
        "id": "QqdrzAMjBgBx"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "![picture](https://drive.google.com/uc?export=view&id=1TA3f22fqppMMSwKYze-cAOWKpv7Vx3TB)\n",
        "\n",
        "\n"
      ],
      "metadata": {
        "id": "z1LRbIjluaqi"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "from keras.models import Sequential\n",
        "from keras.layers import Dense\n",
        "\n",
        "def build_model(input_dim, hidden_neurons, output_dim):\n",
        "    model = Sequential([\n",
        "        Dense(hidden_neurons, input_dim=input_dim, activation='relu'),\n",
        "        Dense(output_dim, activation='linear')  # Assuming you want the raw embedding as output\n",
        "    ])\n",
        "    model.compile(optimizer='adam', loss='mse')\n",
        "    return model\n"
      ],
      "metadata": {
        "id": "ONvwvO6yBij5"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "# Build and train the model\n",
        "**Takes long to train - several hours on my machine**\n",
        "\n",
        "**I have saved the trained model (`language_model_2.h5`). To run the trained model you simply have to load the saved model and run it. **"
      ],
      "metadata": {
        "id": "LfcXWzojB6jS"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# Build model\n",
        "nn_model = build_model(25, 10, 5)\n",
        "\n",
        "# Train the model\n",
        "nn_model.fit(X_train, y_train, epochs=20, batch_size=1)\n",
        "\n",
        "from keras.models import load_model\n",
        "\n",
        "# Save the trained model\n",
        "nn_model.save('language_model_2.h5')  # Saves the model to your hard drive"
      ],
      "metadata": {
        "id": "jJqoKqTRBsPx"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "Save the model for later use"
      ],
      "metadata": {
        "id": "vTEkJ3HxL2Z4"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "#Load the trained model"
      ],
      "metadata": {
        "id": "OurjHgMuMBOU"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "from keras.models import load_model\n",
        "\n",
        "# Load the model from the disk\n",
        "loaded_model = load_model('language_model_2.h5')"
      ],
      "metadata": {
        "id": "VuDYOcdMLw47"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "Compute the loss function over the training data."
      ],
      "metadata": {
        "id": "S23Wt2B02yjf"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# You need to prepare your data in the same way it was prepared during model training\n",
        "loss= loaded_model.evaluate(X_train, y_train)\n",
        "print(f\"Loss: {loss}\")"
      ],
      "metadata": {
        "id": "Cseg9raJ0Ha6"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "The above results indicate this is a better language model than version v1 earlier."
      ],
      "metadata": {
        "id": "Q29v1yR728ht"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "#Next Word Prediction\n",
        "Once the model is trained, we can use it for the next word (token) prediction."
      ],
      "metadata": {
        "id": "Wiv4fH5wMJ47"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "from sklearn.metrics.pairwise import cosine_similarity\n",
        "import numpy as np\n",
        "\n",
        "def predict_next_words(model, input_sequence, word_vectors, top_n=5):\n",
        "    # Predict the embedding\n",
        "    prediction = model.predict(np.array([input_sequence]))[0]\n",
        "\n",
        "    # Calculate cosine similarity with all words\n",
        "    all_similarities = cosine_similarity([prediction], word_vectors.vectors)[0]\n",
        "\n",
        "    # Find the top 5 words with the highest similarity\n",
        "    top_indices = np.argsort(-all_similarities)[:top_n]  # Negative for descending order\n",
        "    closest_words = [(word_vectors.index_to_key[i], all_similarities[i]) for i in top_indices]\n",
        "\n",
        "    return closest_words\n"
      ],
      "metadata": {
        "id": "rpXPgRCWMMmE"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "Run the prediction model"
      ],
      "metadata": {
        "id": "JcqIvMm9SnO8"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# Test the loaded model\n",
        "test_sequence = \"who for many years\" # @param {type:\"string\"}\n",
        "test_tokens = word_tokenize(test_sequence)\n",
        "test_embedded = [word2vec_model.wv[word] for word in test_tokens if word in word2vec_model.wv]\n",
        "test_input = np.concatenate(test_embedded[:5])  # Simplified example\n",
        "\n",
        "vector_size = 5 # the model expects 5 words in the prompt\n",
        "\n",
        "# Ensure there are exactly 5 embeddings, pad if fewer\n",
        "if len(test_embedded) < 5:\n",
        "    # Pad with zero-filled vectors\n",
        "    test_embedded += [np.zeros(vector_size) for _ in range(5 - len(test_embedded))]\n",
        "\n",
        "# Flatten the list of embeddings to match input shape, and ensure it's truncated to exactly 5 words\n",
        "test_input = np.concatenate(test_embedded[:5])\n",
        "\n",
        "predicted_words = predict_next_words(loaded_model, test_input, word2vec_model.wv)\n"
      ],
      "metadata": {
        "id": "NCt6q6IwQ1Ij"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "print(\"Predicted next words:\")\n",
        "for word, similarity in predicted_words:\n",
        "    print(f\"{word}\")"
      ],
      "metadata": {
        "id": "9rkbPSfll_1O"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "#Text Generation with Randomness\n",
        "To make the outputted text more interesting, we will inject a bit of randomness."
      ],
      "metadata": {
        "id": "rgUDaVHQBr1K"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "Text generation function"
      ],
      "metadata": {
        "id": "NTZHPf-PTap7"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "def predict_next_words_with_probabilities(model, input_sequence, word_vectors, top_n=20):\n",
        "    # Predict the embedding\n",
        "    prediction = model.predict(np.array([input_sequence]))[0]\n",
        "\n",
        "    # Calculate cosine similarity with all words\n",
        "    all_similarities = cosine_similarity([prediction], word_vectors.vectors)[0]\n",
        "\n",
        "    # Get the top 5 indices and scores\n",
        "    top_indices = np.argsort(-all_similarities)[:top_n]\n",
        "    top_scores = all_similarities[top_indices]\n",
        "\n",
        "    # Convert scores to probabilities using softmax\n",
        "    top_probabilities = np.exp(top_scores) / np.sum(np.exp(top_scores))\n",
        "\n",
        "    # Ensure the probabilities sum to 1\n",
        "    top_probabilities /= top_probabilities.sum()\n",
        "\n",
        "    return [(word_vectors.index_to_key[i], top_probabilities[j]) for j, i in enumerate(top_indices)]\n",
        "\n"
      ],
      "metadata": {
        "id": "001vjNRITlNF"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "def generate_text(model, initial_text, word_vectors, num_words, vector_size=5):\n",
        "    tokens = word_tokenize(initial_text)\n",
        "    current_embeddings = [word_vectors[word] for word in tokens if word in word_vectors]\n",
        "\n",
        "    generated_words = tokens.copy()\n",
        "\n",
        "    for _ in range(num_words):\n",
        "        if len(current_embeddings) < 5:\n",
        "            padded_embeddings = current_embeddings + [np.zeros(vector_size) for _ in range(5 - len(current_embeddings))]\n",
        "        else:\n",
        "            padded_embeddings = current_embeddings[-5:]\n",
        "\n",
        "        input_sequence = np.concatenate(padded_embeddings)\n",
        "\n",
        "        next_word_options = predict_next_words_with_probabilities(model, input_sequence, word_vectors)\n",
        "\n",
        "        words, probabilities = zip(*next_word_options)\n",
        "\n",
        "        # Normalize probabilities to ensure they sum to 1\n",
        "        probabilities = np.array(probabilities)\n",
        "        probabilities /= probabilities.sum()\n",
        "\n",
        "        next_word = np.random.choice(words, p=probabilities)\n",
        "\n",
        "        generated_words.append(next_word)\n",
        "        current_embeddings.append(word_vectors[next_word])\n",
        "\n",
        "    return ' '.join(generated_words)\n",
        "\n"
      ],
      "metadata": {
        "id": "xV4savleSvB4"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "%%capture\n",
        "\n",
        "initial_text = \"who for many years\" # @param {type:\"string\"}\n",
        "num_words_to_generate = 40 # @param {type:\"integer\"}\n",
        "generated_text = generate_text(loaded_model, initial_text, word2vec_model.wv, num_words_to_generate)"
      ],
      "metadata": {
        "id": "EHO-11ZSTou1"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "generated_text"
      ],
      "metadata": {
        "id": "y1oqTJPaUvg1"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "While this is a better language model than the earlier version, given that this is a 'toy' example, the outputs generated are still not very impressive."
      ],
      "metadata": {
        "id": "02HvMRP2-vSE"
      }
    }
  ]
}