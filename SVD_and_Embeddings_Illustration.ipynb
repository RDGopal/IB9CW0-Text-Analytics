{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "provenance": [],
      "authorship_tag": "ABX9TyNaGRUg7G0ZhylH7oLCSefe",
      "include_colab_link": true
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/RDGopal/IB9CW0-Text-Analytics/blob/main/SVD_and_Embeddings_Illustration.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "# Singular Value Decomposition\n",
        "Singular value decomposition (SVD) is a matrix factorization technique that decomposes a matrix into three matrices: a left singular matrix, a diagonal matrix of singular values, and a right singular matrix. The left singular matrix and the right singular matrix are orthogonal matrices, which means that their columns are orthonormal vectors. The diagonal matrix of singular values contains the singular values of the original matrix, which are non-negative real numbers.\n",
        "\n",
        "Suppose we have a dataset containing information about the ratings that various users have given to a set of movies. The dataset can be represented as a matrix, where each row corresponds to a user and each column corresponds to a movie, and the entries of the matrix represent the ratings given by users to the corresponding movies.\n",
        "\n",
        "$$\n",
        "\\begin{bmatrix}\n",
        " & Movie1 & Movie2 & Movie3 & Movie4 \\\\\n",
        "User1\t& 4 &\t2\t&3 &\t3 \\\\\n",
        "User2\t& 1\t& 5\t& 4& \t5\\\\\n",
        "User3\t& 2\t& 3& \t1\t& 1\\\\\n",
        "User4\t& 1\t& 4\t& 2\t& 2\\\\\n",
        "User5\t& 3\t& 5\t& 4\t& 3\\\\\n",
        "\\end{bmatrix}\n",
        "$$\n",
        "\n",
        "The matrix can be re\n",
        "presented as:\n",
        "\n",
        "$$\n",
        "A = \\begin{bmatrix}\n",
        "4 & 2 & 3 & 3 \\\\\n",
        "1 & 5 & 4 & 5 \\\\\n",
        "2 & 3 & 1 & 1 \\\\\n",
        "1 & 4 & 2 & 2 \\\\\n",
        "3 & 5 & 4 & 3\n",
        "\\end{bmatrix}\n",
        "$$\n",
        "\n",
        "Given an $m$ X $n$ matrix $A$ (in the example above $m$=5 and $n$=4), there exist three matrices $U$, $S$, and $V$ such that:\n",
        "\n",
        "$$ A=USV^T$$\n",
        "\n",
        "where:\n",
        "\n",
        "$V^T$ is the transpose of $V$\n",
        "\n",
        "$U$ is an $m$ X $n$ orthogonal matrix, where the columns are the left singular vectors of $A$\n",
        "\n",
        "$S$ is an $n$ X $n$ diagonal matrix, where the diagonal entries are the singular values of $A$\n",
        "\n",
        "$V$ is an $n$ X $n$ orthogonal matrix, where the columns are the right singular vectors of $A$\n",
        "\n",
        "$V^T$ is the transpose of $V$\n",
        "\n",
        "We will represent movie ratings as a matrix and use Singular Value Decomposition (SVD) to create compressed representations of the movies. The code starts by creating a matrix of movie ratings where each row represents a user and each column represents a movie.\n"
      ],
      "metadata": {
        "id": "G29Htsrl88vW"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "import numpy as np\n",
        "\n",
        "# Create a matrix with movie ratings\n",
        "movie_matrix = np.array([\n",
        "    [4, 2, 3, 3],\n",
        "    [1, 5, 4, 5],\n",
        "    [2, 3, 1, 1],\n",
        "    [1, 4, 2, 2],\n",
        "    [3, 5, 4, 3]\n",
        "])\n"
      ],
      "metadata": {
        "id": "sFnbivBjC5Yd"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "## Apply SVD to the Movie Matrix"
      ],
      "metadata": {
        "id": "wiNfwkZkDHxT"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "from numpy.linalg import svd\n",
        "\n",
        "# Perform SVD on the movie matrix\n",
        "U, S, VT = svd(movie_matrix)"
      ],
      "metadata": {
        "id": "yucdtR_wDBKE"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "## Check the Equality by Multiplying $U$, $S$, and $V^T$\n"
      ],
      "metadata": {
        "id": "aEntKke-DXFM"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "S_matrix = np.diag(S)\n",
        "# Multiply U, S, and V^T to get the original matrix\n",
        "reconstructed_matrix = np.dot(U[:, :4], np.dot(S_matrix, VT))\n",
        "print(reconstructed_matrix)\n"
      ],
      "metadata": {
        "id": "ZauQDat6DNMT"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "##Create Lower-Dimensional Representations\n",
        "Next, we create one-dimensional, two-dimensional, and three-dimensional vector representations of the movies. For the one-dimensional case, we extract the first column of $V$ and multiply it with the first element of the diagonal matrix $S$. The resulting movie_vectors are then plotted, showing that movies 2 and 4 are similar."
      ],
      "metadata": {
        "id": "Fl16i_hODzKN"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "### One-dimension"
      ],
      "metadata": {
        "id": "ofoBwlAGD5xM"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# Get the first column of V (from V^T, so it's the first row of VT)\n",
        "V1 = VT[0, :]\n",
        "\n",
        "# Multiply the first singular value with this vector\n",
        "one_dim_representation = S[0] * V1\n",
        "print(one_dim_representation)\n",
        "\n",
        "import matplotlib.pyplot as plt\n",
        "\n",
        "movies = ['Movie 1', 'Movie 2', 'Movie 3', 'Movie 4']\n",
        "# For 1D plot\n",
        "plt.figure(figsize=(10, 3))\n",
        "for i, (x, label) in enumerate(zip(one_dim_representation, movies)):\n",
        "    plt.scatter(x, 0, marker='o')  # Plot each point at x with a baseline of y=0\n",
        "    plt.text(x, 0.05, label, ha='center', va='bottom')  # Adjust text alignment and position\n",
        "\n",
        "plt.title(\"1D Representation of Movies\")\n",
        "plt.xlabel(\"Dimension 1\")\n",
        "plt.yticks([])  # Hide y-axis as it's not meaningful in 1D plot\n",
        "plt.show()\n"
      ],
      "metadata": {
        "id": "7UtZcMvxD879"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "### Two dimensions"
      ],
      "metadata": {
        "id": "5NLxWMbFFPat"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# Get the first two columns of V (first two rows of VT)\n",
        "V2 = VT[:2, :]\n",
        "\n",
        "# Multiply the first two singular values with these vectors\n",
        "two_dim_representation = np.dot(np.diag(S[:2]), V2)\n",
        "print(two_dim_representation.T)\n",
        "\n",
        "\n",
        "# For 2D plot\n",
        "plt.figure(figsize=(8, 6))\n",
        "plt.scatter(two_dim_representation[0, :], two_dim_representation[1, :])\n",
        "for i, txt in enumerate(['Movie 1', 'Movie 2', 'Movie 3', 'Movie 4']):\n",
        "    plt.annotate(txt, (two_dim_representation[0, i], two_dim_representation[1, i]))\n",
        "plt.title(\"2D Representation of Movies\")\n",
        "plt.xlabel(\"Dimension 1\")\n",
        "plt.ylabel(\"Dimension 2\")\n",
        "plt.grid(True)\n",
        "plt.show()"
      ],
      "metadata": {
        "id": "Q78DWS2oFR8V"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "## Three dimensions"
      ],
      "metadata": {
        "id": "aN5VMvE2GX1c"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "import numpy as np\n",
        "import matplotlib.pyplot as plt\n",
        "from mpl_toolkits.mplot3d import Axes3D\n",
        "from numpy.linalg import svd\n",
        "\n",
        "# Extract the first three singular vectors from V (transposed, so take from VT)\n",
        "V3 = VT[:3, :]\n",
        "\n",
        "# Multiply the first three singular values (diagonal matrix) with the corresponding vectors\n",
        "three_dim_representation = np.dot(np.diag(S[:3]), V3)\n",
        "\n",
        "# Create a 3D scatter plot\n",
        "fig = plt.figure(figsize=(10, 8))\n",
        "ax = fig.add_subplot(111, projection='3d')\n",
        "\n",
        "# Movies and their names\n",
        "movies = ['Movie 1', 'Movie 2', 'Movie 3', 'Movie 4']\n",
        "x, y, z = three_dim_representation[0], three_dim_representation[1], three_dim_representation[2]\n",
        "\n",
        "ax.scatter(x, y, z, marker='o', s=100, c='red')\n",
        "\n",
        "# Annotate each point in 3D space\n",
        "for (i, txt) in enumerate(movies):\n",
        "    ax.text(x[i], y[i], z[i], txt, size=12, color='black')\n",
        "\n",
        "ax.set_xlabel('Dimension 1')\n",
        "ax.set_ylabel('Dimension 2')\n",
        "ax.set_zlabel('Dimension 3')\n",
        "ax.set_title('3D Representation of Movies')\n",
        "\n",
        "plt.show()\n"
      ],
      "metadata": {
        "id": "4Anb8oV4GbqN"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "The lower-dimensional vector representations of the movies capture the relationships between the movies based on the patterns of movie ratings provided by the users. The SVD algorithm identifies latent factors that contribute to the patterns of ratings across the movies, and these factors are represented by the vectors. The vectors provide a compressed representation of the original data, capturing the relationships between the movies in a lower-dimensional space. This allows us to visualize these relationships and identify groups of movies that have similar ratings. By reducing the dimensionality of the data, SVD enables us to more easily identify underlying patterns and relationships between the movies, which can be useful for tasks such as recommendation systems.\n",
        "\n",
        "SVD can be used to obtain word embeddings in NLP by applying it to a document-term matrix, where each row represents a document and each column represents a unique term in the corpus. The resulting SVD factorization provides a lower-dimensional representation of the original matrix, with the columns of the U matrix representing the document embeddings and the rows of the V matrix representing the term embeddings. The values in the S matrix represent the importance or weight of each dimension. By taking the top k dimensions with the highest weights, we can obtain a lower-dimensional representation of the original matrix. These term embeddings capture the semantic meaning and relationships between words in the corpus, allowing us to better understand the underlying structure of the language.\n"
      ],
      "metadata": {
        "id": "UtH6lgu_FwRc"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "#Word2Vec\n",
        "In practise, word2vec, a neural network-based approach for generating word embeddings is more commonly used. It is a two-layer neural network that takes in a large corpus of text as input and outputs a vector representation for each word in the vocabulary. The key idea behind word2vec is to learn word embeddings that capture not just syntactic, but also semantic relationships between words.\n",
        "There are two main architectures for word2vec: Continuous Bag of Words (CBOW) and Skip-gram. In CBOW, the model tries to predict the current word based on its context (the surrounding words), while in skip-gram, the model tries to predict the context words based on the current word. Both models use a sliding window approach to define the context.\n",
        "One of the advantages of word2vec over SVD-based approaches is that it can learn more complex relationships between words, beyond just co-occurrence statistics. Additionally, word2vec can capture more nuanced relationships between words, such as analogies (e.g. “man is to woman as king is to queen”). However, word2vec requires more training data and computational resources compared to SVD.\n"
      ],
      "metadata": {
        "id": "VCNdDOBNF310"
      }
    }
  ]
}